{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507788fa-cb65-4c26-a3b6-b933f8d82d4d",
   "metadata": {},
   "source": [
    "## Langchain LLM - Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706cbb5-17e6-4a3f-aac6-d3a0903acf40",
   "metadata": {},
   "source": [
    "1. Llama-2-7b-chat-hf\n",
    "- https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "2. LANGCHAIN 手冊\n",
    "https://python.langchain.com/docs/integrations/llms/\n",
    "\n",
    "3. 學習llm 模組, 如 llm(\"什麼是聯邦式學習?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d99bb-091b-4607-92d1-d4aa24a89c94",
   "metadata": {},
   "source": [
    "## 初始環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c755e8a-6056-4abf-9931-8d6d4e63b127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6a072-8c6c-4498-94aa-2a11321d71d9",
   "metadata": {},
   "source": [
    "## 確認CUDA版本, 以及否能使用GPU\n",
    "若無gpu 請點選右側->已連線->變更執行階段類型->T4 Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9426b-a114-4f33-ae5a-eeb09a95a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145ec44-0b71-41bd-92c0-fefd9affb806",
   "metadata": {},
   "source": [
    "## 安裝套件\n",
    "安裝完成後建議, 點選上方選單, 直接階段->重新啟動工作階段, 確保 library重置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26741fe-c66f-407b-ac81-959529bb7b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install cohere gdown kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q\n",
    "!pip install accelerate bitsandbytes hf_transfer huggingface_hub optimum transformers -q "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01390959-2a9d-4203-a2cc-7c3a689e54fd",
   "metadata": {},
   "source": [
    "### HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bd6ce-37d9-4cc6-8124-96ccf3c28285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF_TOKEN method 1\n",
    "\n",
    "!echo \"HF_TOKEN=hf_xxxxxxxxxxx\" > .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # loads env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089d28c-0f62-4ba9-895a-fd0958679df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HF_TOKEN method 2\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_xxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3513274b-26a0-4414-9127-79a57f4367ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OPENAPI KEY  method 3\n",
    "\n",
    "import os\n",
    "from typing import TextIO\n",
    "from getpass import getpass\n",
    "os.environ[\"HF_TOKEN\"] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131aa253-8cb7-4171-9f32-7923fc3384ea",
   "metadata": {},
   "source": [
    "### LOAD LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261cc729-b6b3-43a8-9d69-2620c9d4016d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOAD LIBRARY\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561e895-1187-405a-92e9-6c2d0eb5d9a8",
   "metadata": {},
   "source": [
    "### Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9fdb4-569c-422c-8c04-bef242980493",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download model\n",
    "mkdir -p /content/Llama-2-7b-chat-hf\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir /content/Llama-2-7b-chat-hf  --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b644634-295d-4baf-9d66-a1d11fdb3edd",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "temperature 的參數值越小，模型就會回傳越確定的結果。如果調高該參數值，大語言模型可能會返回更隨機的結果，也就是說這可能會帶來更多樣化或更具創造性的產出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8bb11-126d-4243-a4b3-f13e3caaf7ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "MODEL_ID = \"/content/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Pipeline\n",
    "#################################################################\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5eca6-b96f-4c1d-a7f7-89a99fecbb7e",
   "metadata": {},
   "source": [
    "### QUESTION to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a2878-965f-4af9-becd-23674f33566d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response=llm(\"什麼是聯邦式學習?\")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
