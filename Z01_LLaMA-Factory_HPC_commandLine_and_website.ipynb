{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORCx8i1o1ZK9",
    "tags": []
   },
   "source": [
    "# LLaMA_Factory WEB (HPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境安裝\n",
    "- 切換到原生 Python 3 (ipykernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "### 0. pipi install libsray savefolder\n",
    "saveFolder=work\n",
    "label=LlamaFactory\n",
    "\n",
    "### 1. IMAGE\n",
    "IMAGE=/work/u00cjz00/nvidia/cuda118/c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel-llama_factory.sif\n",
    "IMAGE_basename=S-${saveFolder}-${label}_$(basename \"$IMAGE\" .sif)\n",
    "\n",
    "### 2. VIRTUAL LIBRARY and BINARY FOLDER\n",
    "libraryFolder=/${saveFolder}/$(whoami)/libraryFolder/${IMAGE_basename}/local/lib\n",
    "libraryFolder_binding=${libraryFolder}:${HOME}/.local/lib\n",
    "binFolder=/${saveFolder}/$(whoami)/libraryFolder/${IMAGE_basename}/local/bin\n",
    "binFolder_binding=${binFolder}:${HOME}/.local/bin\n",
    "rm -rf /${saveFolder}/$(whoami)/libraryFolder/${IMAGE_basename}\n",
    "mkdir -p ${libraryFolder} ${binFolder}\n",
    "\n",
    "# 3. PIP INSTALL SLAVE IPYKERNEL\n",
    "ml libs/singularity/3.10.2\n",
    "singularity exec -B ${libraryFolder_binding} -B ${binFolder_binding} ${IMAGE} pip install -q ipykernel IProgress ipywidgets\n",
    "\n",
    "# 4. IPYKERNEL for IMAGE\n",
    "IPYKERNEL=/work/u00cjz00/slurm_jobs/ipykernel/t2/image_with_ipykernel_local\n",
    "mkdir -p ${HOME}/.local/share/jupyter/kernels/\n",
    "rm -rf ${HOME}/.local/share/jupyter/kernels/${IMAGE_basename}\n",
    "cp -rf ${IPYKERNEL} ${HOME}/.local/share/jupyter/kernels/${IMAGE_basename}\n",
    "chmod -R 755 ${HOME}/.local/share/jupyter/kernels/${IMAGE_basename}\n",
    "IMAGE_desc=$(echo $IMAGE | sed 's_/_\\\\/_g')\n",
    "sed -i \"s/templateSIF/${IMAGE_desc}/g\" ${HOME}/.local/share/jupyter/kernels/${IMAGE_basename}/kernel.json\n",
    "sed -i \"s/templateImage/Image_${IMAGE_basename}/g\" ${HOME}/.local/share/jupyter/kernels/${IMAGE_basename}/kernel.json\n",
    "sed -i \"s@templateLibrayFolder@${libraryFolder_binding}@g\" ${HOME}/.local/share/jupyter/kernels/${IMAGE_basename}/kernel.json\n",
    "sed -i \"s@templateBinFolder@${binFolder_binding}@g\" ${HOME}/.local/share/jupyter/kernels/${IMAGE_basename}/kernel.json\n",
    "\n",
    "# 5. check size\n",
    "du -sh ${libraryFolder}\n",
    "echo /home/$(whoami)/.local/share/jupyter/kernels/${IMAGE_basename}/kernel.json\n",
    "cat /home/$(whoami)/.local/share/jupyter/kernels/${IMAGE_basename}/kernel.json\n",
    "\n",
    "cmd=\"/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity exec --nv --cleanenv \\\n",
    "-B /work -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin \\\n",
    "-B ${libraryFolder}:/home/g00cjz00/.local/lib \\\n",
    "-B ${binFolder}:/home/g00cjz00/.local/bin \\\n",
    "${IMAGE} \\\n",
    "bash -c 'export PATH=\\$PATH:\\$HOME/.local/bin; echo \\$PATH;'\n",
    "\"\n",
    "\n",
    "echo ${cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAnZLFDg1ZLC"
   },
   "source": [
    "## 初始環境設定\n",
    "- 重新reload kernel, 並切換到 kernel Image_S-work-LlamaFactory_c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel\n",
    "- https://zhuanlan.zhihu.com/p/645010851"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UgaDyzu1ZLD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始環境設定\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 套件安裝 Llama-factory 免安裝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 套件安裝 Llama-factory\n",
    "!pip install llmtuner==0.5.3 -q \n",
    "!pip install deepspeed==0.13.4 -q\n",
    "!pip install bitsandbytes==0.42.0 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install \"unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3Wt3onj1ZLH"
   },
   "source": [
    "## 下載 LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf LLaMA-Factory\n",
    "git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 啟動 Llama-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# IP\n",
    "node_ip=$(cat /etc/hosts |grep \"$(hostname -a)\" | awk '{print $1}')\n",
    "# PORT\n",
    "noed_port_genai=$(python -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(('', 0)); addr = s.getsockname(); s.close(); print(addr[1])\")\n",
    "# PROXY\n",
    "proxy_url=/rstudio/${node_ip}/${noed_port_genai}\n",
    "# URL\n",
    "https_url=https://node01.biobank.org.tw${proxy_url}/\n",
    "\n",
    "# SCRIPT FILE\n",
    "cat << EOF >  LLaMA-Factory/src/train_web_demo.py\n",
    "from llmtuner import create_ui\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "#print(\"${https_url}\")\n",
    "\n",
    "def main():\n",
    "    demo = create_ui()\n",
    "    demo.queue()\n",
    "    demo.launch(server_port=${noed_port_genai}, server_name=\"$(hostname -s)\", share=False, inbrowser=True, root_path=\"${proxy_url}\", auth=(\"nchc\", \"nchcorgtw\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "EOF\n",
    "\n",
    "# 參除\n",
    "ps -ef |grep train_web_demo | awk '{print $2}' | xargs kill -9\n",
    "sleep 1\n",
    "\n",
    "#請更新以下 HF_TOKEN\n",
    "echo $https_url\n",
    "echo \"Account: nchc\"\n",
    "echo \"Password: nchcorgtw\"\n",
    "cd LLaMA-Factory\n",
    "CUDA_VISIBLE_DEVICES=1 HF_TOKEN='hf_' nohup python src/train_web_demo.py > ./llamafactory.log 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMD LINE LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd LLaMA-Factory\n",
    "CUDA_VISIBLE_DEVICES=3 python src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
    "    --finetuning_type lora \\\n",
    "    --template default \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset medical \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 100 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --output_dir saves/LLaMA2-7B/lora/train_2024_v1 \\\n",
    "    --fp16 True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --plot_loss True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMDLINE FULL \n",
    "### Deepspeed STAGE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACCELERATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ps -ef |grep 'train_bash.py' | awk '{print $2}' | xargs kill -9\n",
    "sleep 10\n",
    "rm -rf ~/.cache/*\n",
    "cd LLaMA-Factory\n",
    "rm -rf /scratch/g00cjz00/save/LLaMA2-7B/lora/train_2024_accelerate\n",
    "sleep 3\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch --config_file examples/lora_multi_gpu/config.yaml src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
    "    --finetuning_type full \\\n",
    "    --template default \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset medical \\\n",
    "    --cutoff_len 4096 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --output_dir /scratch/g00cjz00/save/LLaMA2-7B/lora/train_2024_v1 \\\n",
    "    --fp16 True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --plot_loss True \n",
    "\n",
    "#> ./medical.log 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEEPSPEED 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ps -ef |grep 'train_bash.py' | awk '{print $2}' | xargs kill -9\n",
    "sleep 3\n",
    "rm -rf ~/.cache/*\n",
    "cd LLaMA-Factory\n",
    "rm -rf /scratch/g00cjz00/save/LLaMA2-7B/lora/train_2024_v1\n",
    "sleep 3\n",
    "\n",
    "deepspeed --num_gpus 4 src/train_bash.py \\\n",
    "    --deepspeed ../deep_speed_stage2_cpu.json \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
    "    --finetuning_type full \\\n",
    "    --template default \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset medical \\\n",
    "    --cutoff_len 4096 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --output_dir /scratch/g00cjz00/save/LLaMA2-7B/lora/train_2024_v1 \\\n",
    "    --fp16 True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --plot_loss True \n",
    "\n",
    "#> ./medical.log 2>&1 &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ps -ef |grep 'train_bash.py' | awk '{print $2}' | xargs kill -9\n",
    "sleep 3\n",
    "rm -rf ~/.cache/*\n",
    "cd LLaMA-Factory\n",
    "rm -rf /scratch/g00cjz00/save/LLaMA2-7B/lora/train_2024_v1\n",
    "sleep 3\n",
    "\n",
    "deepspeed --num_gpus 4 src/train_bash.py \\\n",
    "    --deepspeed ../deep_speed_stage3.json \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path /work/g00cjz00/models/meta-llama/Llama-2-13b-chat-hf \\\n",
    "    --finetuning_type full \\\n",
    "    --template default \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset medical \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --output_dir /scratch/g00cjz00/save/LLaMA2-7B/lora/train_2024_v1 \\\n",
    "    --fp16 True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --plot_loss True \n",
    "\n",
    "#> ./medical.log 2>&1 &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIXTRAL 120G LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ps -ef |grep 'train_bash.py' | awk '{print $2}' | xargs kill -9\n",
    "sleep 3\n",
    "rm -rf ~/.cache/*\n",
    "cd LLaMA-Factory\n",
    "rm -rf /scratch/g00cjz00/save/LLaMA2-7B/lora/train_2024_v1\n",
    "sleep 3\n",
    "\n",
    "nohup deepspeed --num_gpus 4 src/train_bash.py \\\n",
    "    --deepspeed ../deep_speed_stage3.json \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path /work/g00cjz00/models/model/mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template mistral \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset medical \\\n",
    "    --cutoff_len 4096 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --output_dir /scratch/g00cjz00/save/LLaMA2-7B/lora/train_2024_v1 \\\n",
    "    --fp16 True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --plot_loss True > ./medical.log 2>&1 &\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ps -ef |grep 'train_bash.py' | awk '{print $2}' | xargs kill -9\n",
    "sleep 3\n",
    "rm -rf ~/.cache/*\n",
    "cd LLaMA-Factory\n",
    "rm -rf /scratch/g00cjz00/save/LLaMA2-7B/lora/train_2024_v2\n",
    "sleep 3\n",
    "\n",
    "#nohup deepspeed  --num_gpus 2 src/train_bash.py \\\n",
    "nohup deepspeed --include localhost:0,1,2,3 src/train_bash.py \\\n",
    "    --deepspeed examples/full_multi_gpu/ds_z3_config.json \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path /work/g00cjz00/models/model/mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template mistral \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset medical \\\n",
    "    --cutoff_len 4096 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 100000 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --output_dir /scratch/g00cjz00/save/LLaMA2-7B/lora/train_2024_v2 \\\n",
    "    --fp16 True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --plot_loss True > ./medical4.log 2>&1 &\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 套件安裝 OPENCC\n",
    "```\n",
    "s2t.json Simplified Chinese to Traditional Chinese 簡體到繁體\n",
    "t2s.json Traditional Chinese to Simplified Chinese 繁體到簡體\n",
    "s2tw.json Simplified Chinese to Traditional Chinese (Taiwan Standard) 簡體到臺灣正體\n",
    "tw2s.json Traditional Chinese (Taiwan Standard) to Simplified Chinese 臺灣正體到簡體\n",
    "s2hk.json Simplified Chinese to Traditional Chinese (Hong Kong variant) 簡體到香港繁體\n",
    "hk2s.json Traditional Chinese (Hong Kong variant) to Simplified Chinese 香港繁體到簡體\n",
    "s2twp.json Simplified Chinese to Traditional Chinese (Taiwan Standard) with Taiwanese idiom 簡體到繁體（臺灣正體標準）並轉換爲臺灣常用詞彙\n",
    "tw2sp.json Traditional Chinese (Taiwan Standard) to Simplified Chinese with Mainland Chinese idiom 繁體（臺灣正體標準）到簡體並轉換爲中國大陸常用詞彙\n",
    "t2tw.json Traditional Chinese (OpenCC Standard) to Taiwan Standard 繁體（OpenCC 標準）到臺灣正體\n",
    "hk2t.json Traditional Chinese (Hong Kong variant) to Traditional Chinese 香港繁體到繁體（OpenCC 標準）\n",
    "t2hk.json Traditional Chinese (OpenCC Standard) to Hong Kong variant 繁體（OpenCC 標準）到香港繁體\n",
    "t2jp.json Traditional Chinese Characters (Kyūjitai) to New Japanese Kanji (Shinjitai) 繁體（OpenCC 標準，舊字體）到日文新字體\n",
    "jp2t.json New Japanese Kanji (Shinjitai) to Traditional Chinese Characters (Kyūjitai) 日文新字體到繁體（OpenCC 標準，舊字體）\n",
    "tw2t.json Traditional Chinese (Taiwan standard) to Traditional Chinese 臺灣正體到繁體（OpenCC 標準）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opencc\n",
    "!pip install opencc-python-reimplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import opencc #繁体简体互转\n",
    "from datasets import load_dataset\n",
    "import opencc #繁体简体互转\n",
    "#op_cc=opencc.OpenCC('s2t')\n",
    "op_cc=opencc.OpenCC('s2twp')\n",
    "\n",
    "\n",
    "\n",
    "# 讀取數據集，take可以取得該數據集前n筆資料\n",
    "dataset = load_dataset(\"michaelwzhu/ChatMed_Consult_Dataset\", split=\"train\", streaming=True,encoding='utf-8')\n",
    "\n",
    "# 提取所需欄位並建立新的字典列表\n",
    "limit=0\n",
    "extracted_data = []\n",
    "for example in dataset:\n",
    "    extracted_example = {\n",
    "        \"instruction\": op_cc.convert(\"现在你是一名专业的中医医生，请用你的专业知识提供详尽而清晰的关于中医问题的回答。\"),\n",
    "        \"input\":  op_cc.convert(example[\"query\"]),\n",
    "        \"output\":  op_cc.convert(example[\"response\"])\n",
    "    }\n",
    "    extracted_data.append(extracted_example)\n",
    "    if len(extracted_data) == limit:\n",
    "        break\n",
    "\n",
    "# 指定 JSON 文件名稱\n",
    "json_filename = \"data.json\"\n",
    "\n",
    "# 寫入 JSON 文件\n",
    "with open(json_filename, \"w\") as json_file:\n",
    "    json.dump(extracted_data, json_file, indent=4)\n",
    "\n",
    "print(f\"數據已提取並保存為 {json_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 所有資料內容\n",
    "import pandas as pd\n",
    "df = pd. read_json ( 'data.json' )\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 取出前一萬筆, 並儲存\n",
    "import pandas as pd\n",
    "df = pd. read_json ( 'data.json' )\n",
    "dataset_df_10k = df[:10000]\n",
    "dataset_df_10k.to_json('LLaMA-Factory/data/medicalQA.json', orient='records')\n",
    "dataset_df_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(dataset_df_10k[0:100].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sha1sum , open LLaMA-Factory/data/dataset_info.json\n",
    "!sha1sum LLaMA-Factory/data/medicalQA.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "\"medical\": {\n",
    "    \"file_name\": \"medicalQA.json\",\n",
    "    \"file_sha1\": \"07ffc21c67eeee91501332123be8c640259d0607\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"instruction\",\n",
    "      \"query\": \"input\",\n",
    "      \"response\": \"output\"\n",
    "    }\n",
    "  },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEMO\n",
    "```\n",
    "/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\n",
    "```\n",
    "```\n",
    "你是人工智慧助理，以下是用戶和人工智能助理之間的對話。你要對用戶的問題提供有用、安全、詳細和禮貌的回答。USER: {user} ASSISTANT:\n",
    "```\n",
    "```\n",
    "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {user} ASSISTANT:\n",
    "```\n",
    "```\n",
    "learning_rate: 5e-05\n",
    "distributed_type: multi-GPU\n",
    "optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
    "lr_scheduler_type: cosine\n",
    "lr_scheduler_warmup_ratio: 0.03\n",
    "num_epochs: 5.0\n",
    "```\n",
    "```\n",
    "https://zhuanlan.zhihu.com/p/641960340\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SINGULARITY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ps -ef |grep 'train_bash.py' | awk '{print $2}' | xargs kill -9\n",
    "mkdir -p /scratch/$(whoami)/home/$(whoami) /scratch/$(whoami)/tmp\n",
    "mkdir -p /work/$(whoami)/llama-factory/save\n",
    "rm -rf /scratch/$(whoami)/home/$(whoami)/.cache/*\n",
    "rm -rf /work/$(whoami)/llama-factory/save/demo\n",
    "sleep 3\n",
    "\n",
    "cat << EOF > /scratch/$(whoami)/tmp/demo.sh\n",
    "#!/bin/bash\n",
    "cd /app\n",
    "deepspeed --num_gpus 4 src/train_bash.py \\\n",
    "    --deepspeed /app/examples/full_multi_gpu/ds_z3_config.json \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
    "    --dataset alpaca_gpt4_en \\\n",
    "    --dataset_dir data \\\n",
    "    --template default \\\n",
    "    --finetuning_type full \\\n",
    "    --output_dir /work/$(whoami)/llama-factory/save/demo \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 3000 \\\n",
    "    --val_size 0.1 \\\n",
    "    --plot_loss \\\n",
    "    --fp16\n",
    "EOF\n",
    "\n",
    "chmod 755 /scratch/$(whoami)/tmp/demo.sh\n",
    "\n",
    "ml libs/singularity/3.10.2\n",
    "singularity \\\n",
    "exec -C --nv \\\n",
    "-B /scratch/$(whoami)/home/$(whoami):/home/$(whoami) \\\n",
    "-B /scratch/$(whoami)/tmp:/tmp \\\n",
    "-B /work/$(whoami)/llama-factory/save \\\n",
    "-B /work/u00cjz00/slurm_jobs/github/models \\\n",
    "/work/u00cjz00/nvidia/cuda118/c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel-llama_factory.sif \\\n",
    "/tmp/demo.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLURM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat << \\\\EOF > ~/demo.sh\n",
    "#!/bin/bash\n",
    "\n",
    "ps -ef |grep 'train_bash.py' | awk '{print $2}' | xargs kill -9\n",
    "mkdir -p /scratch/$(whoami)/home/$(whoami) /scratch/$(whoami)/tmp\n",
    "mkdir -p /work/$(whoami)/llama-factory/save\n",
    "rm -rf /scratch/$(whoami)/home/$(whoami)/.cache/*\n",
    "rm -rf /work/$(whoami)/llama-factory/save/demo\n",
    "sleep 3\n",
    "\n",
    "cat << EOF > /scratch/$(whoami)/tmp/demo.sh\n",
    "#!/bin/bash\n",
    "cd /app\n",
    "deepspeed --num_gpus 4 src/train_bash.py \\\n",
    "    --deepspeed /app/examples/full_multi_gpu/ds_z3_config.json \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
    "    --dataset alpaca_gpt4_en \\\n",
    "    --dataset_dir data \\\n",
    "    --template default \\\n",
    "    --finetuning_type full \\\n",
    "    --output_dir /work/$(whoami)/llama-factory/save/demo \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --eval_steps 100 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 3000 \\\n",
    "    --val_size 0.1 \\\n",
    "    --plot_loss \\\n",
    "    --fp16\n",
    "EOF\n",
    "\n",
    "chmod 755 /scratch/$(whoami)/tmp/demo.sh\n",
    "\n",
    "ml libs/singularity/3.10.2\n",
    "singularity \\\n",
    "exec -C --nv \\\n",
    "-B /scratch/$(whoami)/home/$(whoami):/home/$(whoami) \\\n",
    "-B /scratch/$(whoami)/tmp:/tmp \\\n",
    "-B /work/$(whoami)/llama-factory/save \\\n",
    "-B /work/u00cjz00/slurm_jobs/github/models \\\n",
    "/work/u00cjz00/nvidia/cuda118/c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel-llama_factory.sif \\\n",
    "/tmp/demo.sh\n",
    "\n",
    "\\EOF\n",
    "\n",
    "chmod 755 ~/demo.sh\n",
    "\n",
    "\n",
    "\n",
    "cat << EOF >  ~/demo.slurm\n",
    "#!/work/u00cjz00/binary/bash5.0/bin/bash\n",
    "#SBATCH -A GOV109189                                                    ### project number, Example MST109178\n",
    "#SBATCH -J _t2demo_                                                     ### Job name, Exmaple jupyterlab\n",
    "#SBATCH -p gp4d                                                         ### Partition Name, Example ngs1gpu\n",
    "#SBATCH --nodes=1                                                       ### Nodes, Default 1, node number\n",
    "#SBATCH --ntasks-per-node=1                                             ### Tasks, Default 1, per node tasks\n",
    "#SBATCH -c 16                                                            ### Cores assigned to each task, Example 4\n",
    "#SBATCH --gres=gpu:4                                                    ### GPU number, Example gpu:1\n",
    "#SBATCH --time=0-1:00:00                                                ### Runnung time, days-hours:minutes:seconds or hours:minutes:seconds\n",
    "#SBATCH -o genai_%j.out                                                 ### Log folder, Here %j is job ID\n",
    "#SBATCH -e genai_%j.err                                                 ### Log folder, Here %j is job ID\n",
    "\n",
    "\n",
    "~/demo.sh\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat ~/demo.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat ~/demo.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_S-work-LlamaFactory_c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel-llama_factory",
   "language": "python",
   "name": "s-work-llamafactory_c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel-llama_factory"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
