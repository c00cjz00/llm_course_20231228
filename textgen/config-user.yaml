Taiwan-LLM-7B-v2.1-chat-Q8_0.gguf$:
  loader: llama.cpp
  cpu: false
  threads: 4
  threads_batch: 0
  n_batch: 512
  no_mmap: false
  mlock: false
  no_mul_mat_q: false
  n_gpu_layers: 128
  tensor_split: ''
  n_ctx: 4096
  compress_pos_emb: 1
  alpha_value: 1
  rope_freq_base: 0
  numa: false
Llama-2-7B-Chat-GPTQ$:
  loader: ExLlamav2_HF
  cpu_memory: 0
  auto_devices: false
  disk: false
  cpu: false
  trust_remote_code: false
  no_use_fast: false
  wbits: '4'
  groupsize: '128'
  triton: false
  desc_act: false
  no_inject_fused_attention: false
  no_inject_fused_mlp: false
  no_use_cuda_fp16: false
  disable_exllama: false
  gpu_memory_0: 0
  cfg_cache: false
  no_flash_attn: false
  cache_8bit: false
  gpu_split: ''
  max_seq_len: 4096
  compress_pos_emb: 1
  alpha_value: 1
Llama-2-7b-chat-hf$:
  loader: Transformers
  cpu_memory: 0
  auto_devices: false
  disk: false
  cpu: false
  bf16: false
  load_in_8bit: false
  trust_remote_code: false
  no_use_fast: false
  use_flash_attention_2: false
  load_in_4bit: true
  compute_dtype: float16
  quant_type: nf4
  use_double_quant: false
  disable_exllama: false
  compress_pos_emb: 1
  alpha_value: 1
  rope_freq_base: 0
  gpu_memory_0: 0
TheBloke_Orca-2-7B-GPTQ$:
  loader: ExLlamav2_HF
  no_use_fast: false
  cfg_cache: false
  gpu_split: ''
  max_seq_len: 4096
  compress_pos_emb: 1
  alpha_value: 1
  rope_freq_base: 0
  cpu_memory: 0
  auto_devices: false
  disk: false
  cpu: false
  trust_remote_code: false
  wbits: '4'
  groupsize: '128'
  triton: false
  desc_act: false
  no_inject_fused_attention: false
  no_inject_fused_mlp: false
  no_use_cuda_fp16: false
  disable_exllama: false
  gpu_memory_0: 0
  no_flash_attn: false
  cache_8bit: false
