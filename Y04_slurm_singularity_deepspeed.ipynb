{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa50227a-c93c-41b2-a551-30e9a036b672",
   "metadata": {},
   "source": [
    "## DEEPSPEED + SLRUM + SINGULARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96ccdc-ea71-4135-a2e7-1baf283881d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始環境設定\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085fadc-b295-41db-95a3-7321e7d58ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 套件\n",
    "!pip install cohere gdown kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q\n",
    "!pip install accelerate bitsandbytes hf_transfer huggingface_hub optimum transformers==4.36.2 -q \n",
    "!pip install appdirs black black[jupyter] datasets fire loralib sentencepiece gradio==3.48.0 -q\n",
    "!pip install fastapi jieba matplotlib nltk peft==0.7.0 protobuf pydantic rouge-chinese scipy sse-starlette trl==0.7.6 uvicorn -q \n",
    "!pip install deepspeed -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcd1b3e-4a1c-422a-9d1b-55fee195d724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << \\EOF >  demo.script\n",
    "## Method5 DEEPSPEED + Torch.Distributed.Run (singularity/notebook)\n",
    "export GPUS_PER_NODE=2\n",
    "nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIS ) )\n",
    "nodes_array=($nodes)\n",
    "export MASTER_ADDR=${nodes_array[0]}\n",
    "export MASTER_PORT=29003\n",
    "export DS_CONFIG=\"ds_config_zero3.json\"\n",
    "export MODEL_ID=\"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\"\n",
    "export DATASET=\"alpaca_gpt4_zh\"\n",
    "export OUTPUT_DIR=\"saves/LLaMA2-7B-Chat/lora/03_sft\"\n",
    "export MAX_SAMPLE=5000\n",
    "\n",
    "\n",
    "## CLEAN CACHE\n",
    "ps -ef |grep  train | awk '{print $2}' | xargs kill -9\n",
    "rm -rf /work/g00cjz00/github/LLaMA-Factory/${OUTPUT_DIR}\n",
    "\n",
    "## RUN (chage kernel to Python3 or your Image kernel)\n",
    "## If you use Image kernel, unmark next two line, and mark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "## If you use python kernel, mark next two line, and unmark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "\n",
    "/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity run --nv --cleanenv -B /work -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/lib:/home/g00cjz00/.local/lib -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/bin:/home/g00cjz00/.local/bin -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin /work/u00cjz00/nvidia/pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv.sif \\\n",
    "bash -c \"export PATH=\\$PATH:\\$HOME/.local/bin; \\\n",
    "    cd /work/g00cjz00/github/LLaMA-Factory; \\\n",
    "    python3 -m torch.distributed.run \\\n",
    "    --nproc_per_node ${GPUS_PER_NODE} \\\n",
    "    --nnodes ${SLURM_NNODES} \\\n",
    "    --node_rank ${SLURM_PROCID} \\\n",
    "    --master_addr ${MASTER_ADDR} \\\n",
    "    --master_port ${MASTER_PORT} \\\n",
    "    src/train_bash.py \\\n",
    "    --deepspeed ${DS_CONFIG} \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path ${MODEL_ID} \\\n",
    "    --dataset ${DATASET} \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir path_to_sft_checkpoint \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16 \\\n",
    "    --max_samples ${MAX_SAMPLE} \\\n",
    "    --output_dir ${OUTPUT_DIR}\n",
    "\"\n",
    "EOF\n",
    "\n",
    "\n",
    "## SRUN\n",
    "srun -A GOV109189 --partition=gp4d --mpi=pmi2 --gres=gpu:1 --nodes=2 -c 4 --ntasks-per-node=1 --job-name=run_llama -o genai_%j.out -e genai_%j.err bash demo.script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c571fb14-5b0c-4012-8d8b-672f11113bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << \\EOF >  demo.script\n",
    "## Method5 DEEPSPEED + Torch.Distributed.Run (singularity/notebook)\n",
    "export GPUS_PER_NODE=2\n",
    "nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIS ) )\n",
    "nodes_array=($nodes)\n",
    "export MASTER_ADDR=${nodes_array[0]}\n",
    "export MASTER_PORT=29003\n",
    "export DS_CONFIG=\"ds_config_zero3.json\"\n",
    "export MODEL_ID=\"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\"\n",
    "export DATASET=\"alpaca_gpt4_zh\"\n",
    "export OUTPUT_DIR=\"saves/LLaMA2-7B-Chat/lora/03_sft\"\n",
    "export MAX_SAMPLE=5000\n",
    "\n",
    "\n",
    "## CLEAN CACHE\n",
    "ps -ef |grep  train | awk '{print $2}' | xargs kill -9\n",
    "rm -rf /work/g00cjz00/github/LLaMA-Factory/${OUTPUT_DIR}\n",
    "\n",
    "## RUN (chage kernel to Python3 or your Image kernel)\n",
    "## If you use Image kernel, unmark next two line, and mark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "## If you use python kernel, mark next two line, and unmark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "\n",
    "/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity run --nv --cleanenv -B /work -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/lib:/home/g00cjz00/.local/lib -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/bin:/home/g00cjz00/.local/bin -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin /work/u00cjz00/nvidia/pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv.sif \\\n",
    "bash -c \"export PATH=\\$PATH:\\$HOME/.local/bin; \\\n",
    "    cd /work/g00cjz00/github/LLaMA-Factory; \\\n",
    "    python3 -m torch.distributed.run \\\n",
    "    --nproc_per_node ${GPUS_PER_NODE} \\\n",
    "    --nnodes ${SLURM_NNODES} \\\n",
    "    --node_rank ${SLURM_PROCID} \\\n",
    "    --master_addr ${MASTER_ADDR} \\\n",
    "    --master_port ${MASTER_PORT} \\\n",
    "    src/train_bash.py \\\n",
    "    --deepspeed ${DS_CONFIG} \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path ${MODEL_ID} \\\n",
    "    --dataset ${DATASET} \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir path_to_sft_checkpoint \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16 \\\n",
    "    --max_samples ${MAX_SAMPLE} \\\n",
    "    --output_dir ${OUTPUT_DIR}\n",
    "\"\n",
    "EOF\n",
    "\n",
    "\n",
    "cat << \\EOF >  demo.slurm\n",
    "#!/work/u00cjz00/binary/bash5.0/bin/bash\n",
    "#SBATCH -A GOV109189                                                    ### project number, Example MST109178\n",
    "#SBATCH -J _t2demo_                                                     ### Job name, Exmaple jupyterlab\n",
    "#SBATCH -p gp4d                                                         ### Partition Name, Example ngs1gpu\n",
    "#SBATCH --nodes=8                                                       ### Nodes, Default 1, node number\n",
    "#SBATCH --ntasks-per-node=1                                             ### Tasks, Default 1, per node tasks\n",
    "#SBATCH -c 8                                                            ### Cores assigned to each task, Example 4\n",
    "#SBATCH --gres=gpu:2                                                    ### GPU number, Example gpu:1\n",
    "#SBATCH --time=0-1:00:00                                                ### Runnung time, days-hours:minutes:seconds or hours:minutes:seconds\n",
    "#SBATCH -o genai_%j.out                                                 ### Log folder, Here %j is job ID\n",
    "#SBATCH -e genai_%j.err                                                 ### Log folder, Here %j is job ID\n",
    "\n",
    "\n",
    "srun --mpi=pmi2 bash demo.script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
