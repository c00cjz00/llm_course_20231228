{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff818959-0fd5-4a28-b09d-8af2b6e47bfe",
   "metadata": {
    "id": "ff818959-0fd5-4a28-b09d-8af2b6e47bfe"
   },
   "source": [
    "## ##  Finetuning, Aloaca Lora Training\n",
    "https://huggingface.co/docs/transformers/main/peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44fa40-2323-4693-9fb7-f35a937e1d26",
   "metadata": {
    "id": "6e44fa40-2323-4693-9fb7-f35a937e1d26"
   },
   "source": [
    "## 初始環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3285ba4-6ab8-450d-84e2-dbb8e287a93f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "a3285ba4-6ab8-450d-84e2-dbb8e287a93f",
    "outputId": "23d41dd7-235d-4561-bd02-ba29c7d4c2bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456fd3a-842d-4ac0-9d42-1343c1a7e53b",
   "metadata": {
    "id": "9456fd3a-842d-4ac0-9d42-1343c1a7e53b"
   },
   "source": [
    "## 安裝套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cae59-9f20-4b4c-a2a9-654492990069",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f5cae59-9f20-4b4c-a2a9-654492990069",
    "outputId": "0833f79c-02f1-4da2-8c14-0f811fc7b5f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/c00cjz00/alpaca-lora.git alpaca-lora_training_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4jsGEWJKvsuU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4jsGEWJKvsuU",
    "outputId": "c6c80779-3530-405f-e209-6ab3fdbf49bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd alpaca-lora_training_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328a5c1-9337-4cdc-9a7b-6f7595c8b9f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4328a5c1-9337-4cdc-9a7b-6f7595c8b9f7",
    "outputId": "76ad0fe4-a5a2-4133-c21f-ed23f9798f2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install cohere gdown kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q\n",
    "!pip install accelerate bitsandbytes hf_transfer huggingface_hub optimum transformers -q \n",
    "!pip install appdirs black black[jupyter] datasets fire loralib sentencepiece gradio -q\n",
    "!pip install git+https://github.com/huggingface/peft.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04509ba8-3f2d-4975-b204-746be54aede4",
   "metadata": {},
   "source": [
    "### 確認CUDA版本, 以及否能使用GPU\n",
    "若無gpu 請點選右側->已連線->變更執行階段類型->T4 Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1lTV6kF8uqxz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lTV6kF8uqxz",
    "outputId": "d0135e00-4a83-40f3-bf70-7097c4649823",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0d8b0f-871f-4973-b1df-c4c03dbbca21",
   "metadata": {},
   "source": [
    "### 預設資料內容, 並取出1000筆存成新檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XxQJBpMQvefg",
   "metadata": {
    "id": "XxQJBpMQvefg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 所有資料內容\n",
    "import pandas as pd\n",
    "df = pd. read_json ( 'school_math_30000.json' )\n",
    "# 取出前一千筆, 並儲存\n",
    "dataset_df_1k = df[:1000]\n",
    "dataset_df_1k.to_json('data_1k.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e06469-01b2-4877-b59c-7fb76797a4c6",
   "metadata": {},
   "source": [
    "### 關閉wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797eaf0-d162-4d21-aee6-7bbfe3130a12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wandb offline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7cce0a-6263-4315-99c4-c938a7aa5aed",
   "metadata": {},
   "source": [
    "### 訓練開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662c8a7-e587-49aa-8487-4001a23256e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7662c8a7-e587-49aa-8487-4001a23256e0",
    "outputId": "37bb73bb-0dc9-4f23-b0c8-1bc0b3e2be0a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python finetune.py \\\n",
    "    --base_model 'openlm-research/open_llama_3b_v2' \\\n",
    "    --data_path './data_1k.json' \\\n",
    "    --output_dir './data_1k' \\\n",
    "    --batch_size 16 \\\n",
    "    --micro_batch_size 16 \\\n",
    "    --num_epochs 2 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --cutoff_len 512 \\\n",
    "    --val_set_size 500 \\\n",
    "    --lora_r 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --lora_target_modules '[q_proj,v_proj]' \\\n",
    "    --train_on_inputs \\\n",
    "    --group_by_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c2aeb-06ee-4527-af08-36f19b3d2e6f",
   "metadata": {},
   "source": [
    "### 確認結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c45c9-404d-49fa-ab89-94479d8eaa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers.utils import is_accelerate_available, is_bitsandbytes_available\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "ALPACA_TEMPLATE = (\n",
    "    \"Below is an instruction that describes a task, paired with an input that provides \"\n",
    "    \"further context. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_adapted_hf_generation_pipeline(\n",
    "    base_model_name,\n",
    "    lora_model_name,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 1.,\n",
    "    max_tokens: int = 512,\n",
    "    batch_size: int = 16,\n",
    "    device: str = \"cuda\",\n",
    "    load_in_8bit: bool = True,\n",
    "    generation_kwargs: Optional[dict] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a huggingface model & adapt with PEFT.\n",
    "    Borrowed from https://github.com/tloen/alpaca-lora/blob/main/generate.py\n",
    "    \"\"\"\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        if not is_accelerate_available():\n",
    "            raise ValueError(\"Install `accelerate`\")\n",
    "    if load_in_8bit and not is_bitsandbytes_available():\n",
    "            raise ValueError(\"Install `bitsandbytes`\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    task = \"text-generation\"\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            load_in_8bit=load_in_8bit,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    elif device == \"mps\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name, device_map={\"\": device}, low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            device_map={\"\": device},\n",
    "        )\n",
    "\n",
    "    # unwind broken decapoda-research config\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    model.config.bos_token_id = 1\n",
    "    model.config.eos_token_id = 2\n",
    "\n",
    "    if not load_in_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generation_kwargs = generation_kwargs if generation_kwargs is not None else {}\n",
    "    config = GenerationConfig(\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    pipe = pipeline(\n",
    "        task,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=16, # TODO: make a parameter\n",
    "        generation_config=config,\n",
    "        framework=\"pt\",\n",
    "    )\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce53fc7-0cf3-4a1e-9c51-f870bfec2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = load_adapted_hf_generation_pipeline(\n",
    "    base_model_name=\"openlm-research/open_llama_3b_v2\",\n",
    "    lora_model_name=\"./data_1k\"\n",
    ")\n",
    "prompt_template = ALPACA_TEMPLATE.format(\n",
    "    instruction=\"請回答以下數學問題\",\n",
    "    input=\"小明有14顆糖果，他送給小紅5顆，還給小王4顆，請問他現在手中還剩幾顆糖果?\"\n",
    ")\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
