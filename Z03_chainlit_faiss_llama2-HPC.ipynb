{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6EMFg6fTWei"
   },
   "source": [
    "# MEDICAL CHATBOT\n",
    "CHAINLIT+PDF+FAISS_DB+LLAMA2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始環境設定\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 確認CUDA版本, 以及否能使用GPU\n",
    "若無gpu 請點選右側->已連線->變更執行階段類型->T4 Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安裝 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install cohere gdown kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q\n",
    "!pip install accelerate bitsandbytes chainlit faiss-cpu hf_transfer huggingface_hub optimum transformers -q\n",
    "#!pip install auto-gptq -q\n",
    "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  -q # Use cu117 if on CUDA 11.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 文件下載"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/book/\n",
    "!gdown 1pUDgs3YMnlr8See8Rld3L1ZRiTeeOlMM -O data/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: create ingest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << \\EOF >  ingest.py\n",
    "\n",
    "# RUN: python3 ingest.py\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "DATA_PATH = 'data/book/'\n",
    "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "\n",
    "# Create vector database\n",
    "def create_vector_db():\n",
    "    loader = DirectoryLoader(DATA_PATH,\n",
    "                             glob='*.pdf',\n",
    "                             loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                                   chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_vector_db()\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料入庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQ4Jv_z2hRh6"
   },
   "outputs": [],
   "source": [
    "!python3 ingest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step2: create model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << \\EOF >  model.py\n",
    "# RUN: chainlit run model.py\n",
    "\n",
    "# 01: CONFIGURE MODEL_ID and DB PATHs\n",
    "#MODEL_ID = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "MODEL_ID = \"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7B-Chat-GPTQ\"\n",
    "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "\n",
    "# 02: Load LIBRARY\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "import chainlit as cl\n",
    "import transformers\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "#from transformers import AutoTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import os\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "# 03: custom_prompt_template\n",
    "custom_prompt_template3 = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "custom_prompt_template=\"\"\"<|im_start|>system\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Question: {question}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt():\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "# 04: Retrieval QA Chain\n",
    "def retrieval_qa_chain(llm, prompt, db):\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=db.as_retriever(search_kwargs={'k': 5}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt': prompt}\n",
    "                                       )\n",
    "    return qa_chain\n",
    "\n",
    "def load_llm():\n",
    "    # 04: LLM模型 GPTQ\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    llm=HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0.7})\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# 05: QA Model Function\n",
    "def qa_bot():\n",
    "#    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#                                       model_kwargs={'device': 'cpu'})\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='/work/u00cjz00/slurm_jobs/github/models/embedding/all-MiniLM-L6-v2')\n",
    "    db = FAISS.load_local(DB_FAISS_PATH, embeddings)\n",
    "    llm = load_llm()\n",
    "    qa_prompt = set_custom_prompt()\n",
    "    qa = retrieval_qa_chain(llm, qa_prompt, db)\n",
    "\n",
    "    return qa\n",
    "\n",
    "# 06: output function\n",
    "def final_result(query):\n",
    "    qa_result = qa_bot()\n",
    "    response = qa_result({'query': query})\n",
    "    return response\n",
    "\n",
    "# 07: chainlit code\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    chain = qa_bot()\n",
    "    msg = cl.Message(content=\"Starting the bot...\")\n",
    "    await msg.send()\n",
    "    msg.content = \"Hi, Welcome to Medical Bot. What is your query?\"\n",
    "    await msg.update()\n",
    "\n",
    "    cl.user_session.set(\"chain\", chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    chain = cl.user_session.get(\"chain\") \n",
    "    cb = cl.AsyncLangchainCallbackHandler(\n",
    "        stream_final_answer=True, answer_prefix_tokens=[\"FINAL\", \"ANSWER\"]\n",
    "    )\n",
    "    cb.answer_reached = True\n",
    "    res = await chain.acall(message.content, callbacks=[cb])\n",
    "    answer = res[\"result\"]\n",
    "    sources = res[\"source_documents\"]\n",
    "    source_elements = []\n",
    "\n",
    "    found_sources = []\n",
    "\n",
    "    for i, doc in enumerate(sources):\n",
    "        page_content=(res['source_documents'][i].page_content)\n",
    "        page=(res['source_documents'][i].metadata[\"page\"])\n",
    "        source=res['source_documents'][i].metadata[\"source\"]\n",
    "        file = os.path.basename(source) \n",
    "        #print(\"SOURCE: \"+file+\", PAGE: \"+str(page) ) \n",
    "        source_name=f\"source_{i}\"\n",
    "        \n",
    "        found_sources.append(source_name)\n",
    "        text=\"[Document: \"+file+\", Page: \"+str(page)+\"]\\n\\nContent: \"+page_content        \n",
    "        source_elements.append(cl.Text(content=text, name=source_name))\n",
    "            \n",
    "\n",
    "\n",
    "    if sources:\n",
    "        #answer += f\"\\n\\n\\n資料來源, Sources:\" + str(sources)\n",
    "        answer += f\"\\nSources: {', '.join(found_sources)}\"\n",
    "    else:\n",
    "        answer += \"\\n\\n\\nNo sources found\"\n",
    "\n",
    "    #await cl.Message(content=answer).send()\n",
    "    await cl.Message(content=answer, elements=source_elements).send()\n",
    "    \n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 執行模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "node_hostname=$(hostname -s)\n",
    "node_ip=$(cat /etc/hosts |grep \"$(hostname -a)\" | awk '{print $1}')\n",
    "# PORT\n",
    "noed_port_genai=$(python -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(('', 0)); addr = s.getsockname(); s.close(); print(addr[1])\")\n",
    "noed_port_genai=36000\n",
    "\n",
    "# PROXY\n",
    "proxy_url=/rstudio/${node_ip}/${noed_port_genai}\n",
    "# URL\n",
    "https_url=https://node01.biobank.org.tw${proxy_url}/\n",
    "\n",
    "# SSH FORWARDING\n",
    "ssh_cmd=\"ssh -o StrictHostKeyChecking=no -o TCPKeepAlive=yes -o ServerAliveCountMax=20 -o ServerAliveInterval=15 -NfL ${noed_port_genai}:${node_hostname}:${noed_port_genai} $(whoami)@t3-c4.nchc.org.tw\"; \n",
    "echo \"SSH:\"\n",
    "echo ${ssh_cmd}\n",
    "echo \"\"\n",
    "ssh_cmd=\"ssh -L ${noed_port_genai}:${node_hostname}:${noed_port_genai} $(whoami)@t3-c4.nchc.org.tw\"\n",
    "echo \"SSH:\"\n",
    "echo ${ssh_cmd}\n",
    "echo \"\"\n",
    "echo \"URL: http://localhost:${noed_port_genai}\"\n",
    "echo \"\"\n",
    "\n",
    "# CHAINLIT\n",
    "chainlit run model.py -w --port ${noed_port_genai} --host ${node_hostname}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DELETE JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfFTVIWKkhgt",
    "outputId": "aa829169-1abe-4286-cf57-08949332ea7a"
   },
   "outputs": [],
   "source": [
    "!ps -ef |grep chainlit | awk '{print $2}' | xargs kill -9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_S_work-genai3_pytorch_2.1.0-cuda11.8-cudnn8-runtime_textgen",
   "language": "python",
   "name": "s_work-genai3_pytorch_2.1.0-cuda11.8-cudnn8-runtime_textgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
