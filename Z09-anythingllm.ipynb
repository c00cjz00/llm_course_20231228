{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "015ef768-3bb0-4a89-863d-380aa18d709b",
   "metadata": {},
   "source": [
    "## DOCKER IMAGE\n",
    "- local-ai: localai/localai:v2.8.2-cublas-cuda11 localai/localai:master-cublas-cuda11 , https://localai.io/basics/getting_started/\n",
    "- node: node:hydrogen-bullseye\n",
    "- anythingllm: mintplexlabs/anythingllm\n",
    "- ollama: ollama/ollama:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7db8a3c0-eeae-41e2-beb9-de8bf91d5f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kill: sending signal to 112968 failed: No such process\n",
      "Error for command \"stop\": no instance found\n",
      "\n",
      "Usage:\n",
      "  singularity [global options...] instance stop [stop options...] [instance]\n",
      "\n",
      "Run 'singularity --help' for more detailed usage information.\n",
      "INFO:    instance started successfully\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ps -ef |grep 'ollama' | awk '{print $2}' | xargs kill -9\n",
    "export SINGULARITYENV_OLLAMA_HOST=\"$(hostname -s):11434\"\n",
    "DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n",
    "mkdir -p ${DIR}/ollama_models\n",
    "ml libs/singularity/3.10.2\n",
    "singularity instance stop ollama\n",
    "singularity instance start --no-home --nv -B /work -B ${DIR}/ollama_models:$HOME/.ollama /work/u00cjz00/nvidia/ollama_latest.sif ollama\n",
    "sleep 5\n",
    "nohup singularity exec instance://ollama ollama serve  > ./ollama.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350ef50-b323-48e0-abdc-c457aca6fee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "# download model\n",
    "export SINGULARITYENV_OLLAMA_HOST=\"$(hostname -s):11434\"\n",
    "/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity exec instance://ollama ollama pull phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3fe4d-d23b-44a5-92ee-d8b1c33ea5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "# copy model\n",
    "DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n",
    "echo \"FROM /work/u00cjz00/slurm_jobs/github/models/Taiwan-LLM-7B-v2.1-chat-Q8_0.gguf\" > ${DIR}/ollama_models/Modelfile_demo\n",
    "export SINGULARITYENV_OLLAMA_HOST=\"$(hostname -s):11434\"\n",
    "/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity exec instance://ollama ollama create Taiwan-LLM-7B-v2.1-chat-Q8_0 -f $HOME/.ollama/Modelfile_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9ccf9130-719e-4d92-a822-3c4ba684a87d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: sending signal to 120514 failed: No such process\n"
     ]
    }
   ],
   "source": [
    "# stop ollama\n",
    "!ps -ef |grep 'ollama' | awk '{print $2}' | xargs kill -9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcda5816-a9c1-4d63-915f-7787ff2ce35f",
   "metadata": {},
   "source": [
    "## 2. 下載 local-ai 進行模型佈屬\n",
    "https://localai.io/basics/getting_started/\n",
    "\n",
    "https://dev.to/worldlinetech/introducing-localai-4gg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0d51da36-b376-4416-9a48-d58e3bf2df36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LocalAI'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf LocalAI\n",
    "git clone https://github.com/mudler/LocalAI.git\n",
    "DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n",
    "mkdir -p ${DIR}/myLocalAI/models ${DIR}/myLocalAI/images\n",
    "cp ${DIR}/LocalAI/.env ${DIR}/myLocalAI/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32dc086b-5390-499c-b7ff-1f8e53a0fb96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kill: sending signal to 208976 failed: No such process\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "port=12346\n",
    "model=\"phi-2\"\n",
    "DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n",
    "export SINGULARITYENV_env_file=\".env\"\n",
    "ml libs/singularity/3.10.2\n",
    "ps -ef |grep 'local-ai' | awk '{print $2}' | xargs kill -9\n",
    "nohup singularity exec --nv -B /work -B /work/u00cjz00/slurm_jobs/github/local-ai_model:/models -B ${DIR}/myLocalAI/images:/tmp/generated/images /work/u00cjz00/nvidia/localai_v2.8.2-cublas-cuda11.sif \\\n",
    "/build/local-ai --address $(hostname -s):${port} --models-path /models --models ${model} > ./local-ai.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67cbeada-bcfb-4f79-93c6-7056e73e0d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g00cjz00 117552  48250  0 21:00 pts/2    00:00:00 /bin/bash -c ps -ef |grep 'local-ai'\n",
      "g00cjz00 117554 117552  0 21:00 pts/2    00:00:00 grep local-ai\n"
     ]
    }
   ],
   "source": [
    "# check job\n",
    "!ps -ef |grep 'local-ai' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a70e25a5-36d7-4010-84b9-90b000726096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: sending signal to 208915 failed: No such process\n"
     ]
    }
   ],
   "source": [
    "# kill job\n",
    "!ps -ef |grep 'local-ai' | awk '{print $2}' | xargs kill -9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6671ad-3f4e-4d46-9afb-025f4ca3bea1",
   "metadata": {},
   "source": [
    "## 2. 下載 anything-llm 進行編譯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9d6af44-9031-4a94-8bf0-2696215b3396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'anything-llm'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf anything-llm\n",
    "#git clone https://github.com/Mintplex-Labs/anything-llm.git\n",
    "git clone https://github.com/c00cjz00/anything-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "96eb5cee-76e5-4e42-82b0-31e119fd0357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << \\EOF >  demo.script\n",
    "\n",
    "DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n",
    "echo \"fix\"\n",
    "sed -i \"s/5.3.0/5.3.1/g\" ${DIR}/anything-llm//server/package.json\n",
    "\n",
    "echo \"Rebuilding Frontend\"\n",
    "cd ${DIR}/anything-llm/frontend && yarn && yarn build\n",
    "\n",
    "echo \"Copying to Sever Public\"\n",
    "rm -rf ${DIR}/anything-llm/server/public\n",
    "cp -r ${DIR}/anything-llm/frontend/dist ${DIR}/anything-llm/server/public\n",
    "\n",
    "echo \"Installing collector dependencies\"\n",
    "cd ${DIR}/anything-llm/collector && yarn\n",
    "\n",
    "echo \"Installing server dependencies & running migrations\"\n",
    "cd ${DIR}/anything-llm/server && yarn\n",
    "cd ${DIR}/anything-llm/server && npx prisma migrate deploy --schema=./prisma/schema.prisma\n",
    "cd ${DIR}/anything-llm/server && npx prisma generate\n",
    "\n",
    "echo \".env\"\n",
    "touch ${DIR}/anything-llm/server/.env\n",
    "\n",
    "echo \"ssl\"\n",
    "cp -rf ${DIR}/anything-llm-patch/server/ssl ${DIR}/anything-llm/server/ssl\n",
    "cp ${DIR}/anything-llm-patch/server/env ${DIR}/anything-llm/server/.env\n",
    "EOF\n",
    "\n",
    "chmod 755 demo.script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a0ba3-e0a4-44ff-a079-d6a0620bbcd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ml libs/singularity/3.10.2\n",
    "singularity exec -B /work /work/u00cjz00/nvidia/node_hydrogen-bullseye.sif ./demo.script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbdc9d2-beff-4135-b2c3-20c66a18bcb8",
   "metadata": {},
   "source": [
    "## 啟動  anything-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3b6dfd87-d56d-4b88-af42-7d0869a413cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ml libs/singularity/3.10.2\n",
    "export SINGULARITYENV_STORAGE_DIR=\"/app/server/storage\"\n",
    "nohup singularity run -B anything-llm:/app /work/u00cjz00/nvidia/anythingllm_latest.sif > ./anything-llm.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f34316c-4f0b-4295-a908-f5df2b649385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!/usr/sbin/lsof -i -P -n |grep node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad76514-5355-425e-a78e-78b3d43b7ef6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 連線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "495569a1-012b-4f2d-aede-34acb4bc1173",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSH:\n",
      "ssh -o StrictHostKeyChecking=no -o TCPKeepAlive=yes -o ServerAliveCountMax=20 -o ServerAliveInterval=15 -NfL 3001:gpn3002:3001 g00cjz00@t3-c4.nchc.org.tw\n",
      "\n",
      "SSH:\n",
      "ssh -L 3001:gpn3002:3001 g00cjz00@t3-c4.nchc.org.tw\n",
      "\n",
      "URL: http://localhost:3001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "node_hostname=$(hostname -s)\n",
    "node_ip=$(cat /etc/hosts |grep \"$(hostname -a)\" | awk '{print $1}')\n",
    "# PORT\n",
    "noed_port_genai=$(python -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(('', 0)); addr = s.getsockname(); s.close(); print(addr[1])\")\n",
    "noed_port_genai=3001\n",
    "\n",
    "# PROXY\n",
    "proxy_url=/rstudio/${node_ip}/${noed_port_genai}\n",
    "# URL\n",
    "https_url=https://node01.biobank.org.tw${proxy_url}/\n",
    "\n",
    "# SSH FORWARDING\n",
    "ssh_cmd=\"ssh -o StrictHostKeyChecking=no -o TCPKeepAlive=yes -o ServerAliveCountMax=20 -o ServerAliveInterval=15 -NfL ${noed_port_genai}:${node_hostname}:${noed_port_genai} $(whoami)@t3-c4.nchc.org.tw\"; \n",
    "echo \"SSH:\"\n",
    "echo ${ssh_cmd}\n",
    "echo \"\"\n",
    "ssh_cmd=\"ssh -L ${noed_port_genai}:${node_hostname}:${noed_port_genai} $(whoami)@t3-c4.nchc.org.tw\"\n",
    "echo \"SSH:\"\n",
    "echo ${ssh_cmd}\n",
    "echo \"\"\n",
    "echo \"URL: http://localhost:${noed_port_genai}\"\n",
    "echo \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ebb58f-3c56-4698-b02d-a742c0387ab9",
   "metadata": {},
   "source": [
    "## DELETE JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "316e62f0-2b69-44a4-ac17-8e3a058c7809",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: sending signal to 7340 failed: Operation not permitted\n",
      "kill: sending signal to 8598 failed: Operation not permitted\n",
      "kill: sending signal to 117599 failed: No such process\n"
     ]
    }
   ],
   "source": [
    "!ps -ef |grep node | awk '{print $2}' | xargs kill -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d16a9-636e-4f16-8465-28b176f02f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
