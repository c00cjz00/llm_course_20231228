{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff818959-0fd5-4a28-b09d-8af2b6e47bfe",
   "metadata": {},
   "source": [
    "##  Finetuning, Create Datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cc24844-1291-4173-a840-bde9764aaffc",
   "metadata": {},
   "source": [
    "#### Reference \n",
    "https://huggingface.co/CHIH-HUNG/llama-2-13b-FINETUNE3_3.3w-r4-q_k_v_o\n",
    "https://huggingface.co/datasets/huangyt/FINETUNE3\n",
    "https://huggingface.co/erhwenkuo\n",
    "https://huggingface.co/datasets/erhwenkuo/school_math_0.25m-zhtw\n",
    "https://huggingface.co/datasets/erhwenkuo/medical_dialogue-chinese-zhtw\n",
    "https://huggingface.co/datasets/DataAgent/medical-qa-instruction-zhtw\n",
    "https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e44fa40-2323-4693-9fb7-f35a937e1d26",
   "metadata": {},
   "source": [
    "## 初始環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3285ba4-6ab8-450d-84e2-dbb8e287a93f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60615989-4324-484c-8896-94c2590797d9",
   "metadata": {},
   "source": [
    "## 確認CUDA版本, 以及否能使用GPU\n",
    "若無gpu 請點選右側->已連線->變更執行階段類型->T4 Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab7ceb8-81f4-48a1-9b85-2806ef72b0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456fd3a-842d-4ac0-9d42-1343c1a7e53b",
   "metadata": {},
   "source": [
    "## 安裝套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9540a73-2762-4ac4-97ee-1a018dc664dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f2282-5719-49f7-bf1c-b51e05050218",
   "metadata": {},
   "source": [
    "### LOAD LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9652c-a06a-492b-8545-dd9714b2fcea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c9f7f-10d3-48fc-86d9-a39cd9eafd58",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9beae9-429c-462c-969c-83aee3276724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 讀取數據集，take可以取得該數據集前n筆資料\n",
    "dataset = load_dataset(\"lavita/ChatDoctor-HealthCareMagic-100k\", split=\"train\", streaming=True)\n",
    "\n",
    "# 提取所需欄位並建立新的字典列表\n",
    "limit=0\n",
    "extracted_data = []\n",
    "for example in dataset:\n",
    "    extracted_example = {\n",
    "        \"instruction\": example[\"instruction\"],\n",
    "        \"input\": example[\"input\"],\n",
    "        \"output\": example[\"output\"]\n",
    "    }\n",
    "    extracted_data.append(extracted_example)\n",
    "    if len(extracted_data) == limit:\n",
    "        break\n",
    "\n",
    "# 指定 JSON 文件名稱\n",
    "json_filename = \"data.json\"\n",
    "\n",
    "# 寫入 JSON 文件\n",
    "with open(json_filename, \"w\") as json_file:\n",
    "    json.dump(extracted_data, json_file, indent=4)\n",
    "\n",
    "print(f\"數據已提取並保存為 {json_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a2c2b-3e75-4163-b26e-b53048ea0204",
   "metadata": {},
   "source": [
    "### 建立本地端資料內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6035d-a970-42ee-a1c0-c65df4688e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 所有資料內容\n",
    "import pandas as pd\n",
    "df = pd. read_json ( 'data.json' )\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669d319-862e-4b8d-bc3e-658a68b68edd",
   "metadata": {},
   "source": [
    "### 取出前一千筆, 並儲存為本地端資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202a361-107d-41ec-9074-245733131289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 取出前一千筆, 並儲存\n",
    "dataset_df_1k = df[:1000]\n",
    "dataset_df_1k.to_json('data_1k.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97687a7d-c9f0-4163-be1c-234315475040",
   "metadata": {},
   "source": [
    "## Finetuning, Aloaca Lora Training\n",
    "https://huggingface.co/docs/transformers/main/peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cae59-9f20-4b4c-a2a9-654492990069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/c00cjz00/alpaca-lora.git alpaca-lora_training_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a7bc2-3e01-456d-b4a3-d9102d1fd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd alpaca-lora_training_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b43ae-e65b-4b8f-a35b-eea1727170aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install cohere gdown kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q\n",
    "!pip install accelerate bitsandbytes hf_transfer huggingface_hub optimum transformers -q \n",
    "!pip install appdirs black black[jupyter] datasets fire loralib sentencepiece gradio -q\n",
    "!pip install git+https://github.com/huggingface/peft.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9cc6b1-a362-4f20-b95f-5f1f91a9a01a",
   "metadata": {},
   "source": [
    "### 訓練開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662c8a7-e587-49aa-8487-4001a23256e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd alpaca-lora_training_v1\n",
    "!cp ../data_1k.json .\n",
    "!wandb offline\n",
    "!python3 finetune.py \\\n",
    "    --base_model 'openlm-research/open_llama_3b_v2' \\\n",
    "    --data_path './data_1k.json' \\\n",
    "    --output_dir './data_1k' \\\n",
    "    --batch_size 16 \\\n",
    "    --micro_batch_size 16 \\\n",
    "    --num_epochs 2 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --cutoff_len 512 \\\n",
    "    --val_set_size 500 \\\n",
    "    --lora_r 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --lora_target_modules '[q_proj,v_proj]' \\\n",
    "    --train_on_inputs \\\n",
    "    --group_by_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7a563-f5af-4d52-afbd-27c530fed61b",
   "metadata": {},
   "source": [
    "### 確認結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec3ed6f-8670-42f2-938e-e1c76434d666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers.utils import is_accelerate_available, is_bitsandbytes_available\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "ALPACA_TEMPLATE = (\n",
    "    \"Below is an instruction that describes a task, paired with an input that provides \"\n",
    "    \"further context. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_adapted_hf_generation_pipeline(\n",
    "    base_model_name,\n",
    "    lora_model_name,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 1.,\n",
    "    max_tokens: int = 512,\n",
    "    batch_size: int = 16,\n",
    "    device: str = \"cuda\",\n",
    "    load_in_8bit: bool = True,\n",
    "    generation_kwargs: Optional[dict] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a huggingface model & adapt with PEFT.\n",
    "    Borrowed from https://github.com/tloen/alpaca-lora/blob/main/generate.py\n",
    "    \"\"\"\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        if not is_accelerate_available():\n",
    "            raise ValueError(\"Install `accelerate`\")\n",
    "    if load_in_8bit and not is_bitsandbytes_available():\n",
    "            raise ValueError(\"Install `bitsandbytes`\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    task = \"text-generation\"\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            load_in_8bit=load_in_8bit,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    elif device == \"mps\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name, device_map={\"\": device}, low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            device_map={\"\": device},\n",
    "        )\n",
    "\n",
    "    # unwind broken decapoda-research config\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    model.config.bos_token_id = 1\n",
    "    model.config.eos_token_id = 2\n",
    "\n",
    "    if not load_in_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generation_kwargs = generation_kwargs if generation_kwargs is not None else {}\n",
    "    config = GenerationConfig(\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    pipe = pipeline(\n",
    "        task,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=16, # TODO: make a parameter\n",
    "        generation_config=config,\n",
    "        framework=\"pt\",\n",
    "    )\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ed306-a4ef-4103-b123-f82adcd40101",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = load_adapted_hf_generation_pipeline(\n",
    "    base_model_name=\"openlm-research/open_llama_3b_v2\",\n",
    "    lora_model_name=\"./data_1k\"\n",
    ")\n",
    "prompt_template = ALPACA_TEMPLATE.format(\n",
    "    instruction=\"請回答以下數學問題\",\n",
    "    input=\"小明有14顆糖果，他送給小紅5顆，還給小王4顆，請問他現在手中還剩幾顆糖果?\"\n",
    ")\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
