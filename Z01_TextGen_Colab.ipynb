{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFQl6-FjSYtY"
   },
   "source": [
    "# oobabooga/text-generation-webui\n",
    "\n",
    "After running both cells, a public gradio URL will appear at the bottom in a few minutes. You can optionally generate an API link.\n",
    "\n",
    "* Project page: https://github.com/oobabooga/text-generation-webui\n",
    "* Gradio server status: https://status.gradio.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7TVVj_z4flw"
   },
   "outputs": [],
   "source": [
    "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
    "\n",
    "#@markdown Press play on the music player that will appear below:\n",
    "%%html\n",
    "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝Training 所需套件\n",
    "!pip install intel-extension-for-pytorch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGQ8BiMuXMDG"
   },
   "outputs": [],
   "source": [
    "#@title 2. Launch the web UI\n",
    "\n",
    "#@markdown If unsure about the branch, write \"main\" or leave it blank.\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "if Path.cwd().name != 'text-generation-webui':\n",
    "  print(\"Installing the webui...\")\n",
    "\n",
    "  !git clone --branch snapshot-2023-12-10 https://github.com/oobabooga/text-generation-webui\n",
    "  %cd text-generation-webui\n",
    "\n",
    "  torver = torch.__version__\n",
    "  print(f\"TORCH: {torver}\")\n",
    "  is_cuda118 = '+cu118' in torver  # 2.1.0+cu118\n",
    "  is_cuda117 = '+cu117' in torver  # 2.0.1+cu117\n",
    "\n",
    "  textgen_requirements = open('requirements.txt').read().splitlines()\n",
    "  if is_cuda117:\n",
    "      textgen_requirements = [req.replace('+cu121', '+cu117').replace('+cu122', '+cu117').replace('torch2.1', 'torch2.0') for req in textgen_requirements]\n",
    "  elif is_cuda118:\n",
    "      textgen_requirements = [req.replace('+cu121', '+cu118').replace('+cu122', '+cu118') for req in textgen_requirements]\n",
    "  with open('temp_requirements.txt', 'w') as file:\n",
    "      file.write('\\n'.join(textgen_requirements))\n",
    "\n",
    "  !pip install -r extensions/openai/requirements.txt --upgrade\n",
    "  !pip install -r temp_requirements.txt --upgrade\n",
    "\n",
    "  print(\"\\033[1;32;1m\\n --> If you see a warning about \\\"previously imported packages\\\", just ignore it.\\033[0;37;0m\")\n",
    "  print(\"\\033[1;32;1m\\n --> There is no need to restart the runtime.\\n\\033[0;37;0m\")\n",
    "\n",
    "  try:\n",
    "    import flash_attn\n",
    "  except:\n",
    "    !pip uninstall -y flash_attn\n",
    "\n",
    "# Parameters\n",
    "model_url = \"\" #@param {type:\"string\"}\n",
    "branch = \"\" #@param {type:\"string\"}\n",
    "command_line_flags = \"\" #@param {type:\"string\"}\n",
    "api = False #@param {type:\"boolean\"}\n",
    "\n",
    "if api:\n",
    "  for param in ['--api', '--public-api']:\n",
    "    if param not in command_line_flags:\n",
    "      command_line_flags += f\" {param}\"\n",
    "\n",
    "model_url = model_url.strip()\n",
    "if model_url != \"\":\n",
    "    if not model_url.startswith('http'):\n",
    "        model_url = 'https://huggingface.co/' + model_url\n",
    "\n",
    "    # Download the model\n",
    "    url_parts = model_url.strip('/').strip().split('/')\n",
    "    output_folder = f\"{url_parts[-2]}_{url_parts[-1]}\"\n",
    "    branch = branch.strip('\"\\' ')\n",
    "    if branch.strip() not in ['', 'main']:\n",
    "        output_folder += f\"_{branch}\"\n",
    "        !python download-model.py {model_url} --branch {branch}\n",
    "    else:\n",
    "        !python download-model.py {model_url}\n",
    "else:\n",
    "    output_folder = \"\"\n",
    "\n",
    "# Start the web UI\n",
    "#cmd = f\"HF_TOKEN='hf_xxxxx' python server.py --share --chat-buttons --gradio-auth nchc:nchc --extensions Training_PRO\"\n",
    "cmd = f\"python server.py --share --chat-buttons --gradio-auth nchc:nchc --extensions Training_PRO\"\n",
    "if output_folder != \"\":\n",
    "    cmd += f\" --model {output_folder}\"\n",
    "cmd += f\" {command_line_flags}\"\n",
    "print(cmd)\n",
    "!$cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dD5d8esTFIe0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuAJgmuoFJI1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
