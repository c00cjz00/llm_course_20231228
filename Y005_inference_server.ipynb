{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe6ee89-f586-45a5-b547-5e2f7fe91fc7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inference Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752f3dd-04c1-4f0f-ae31-f6cd1ac4cea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "761b8058-0de1-40e6-b031-5d9d5365730b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 16 10:11:07 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    43W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    40W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a88a46-9608-431d-abda-df2826bcfaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cohere gdown kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q\n",
    "#!pip install accelerate bitsandbytes hf_transfer huggingface_hub optimum transformers -q \n",
    "#!pip install auto-gptq\n",
    "#!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  -q # Use cu117 if on CUDA 11.7\n",
    "#!pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e611b5-3758-45ae-976e-d649f2029a5d",
   "metadata": {},
   "source": [
    "## SINGULARITY KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779baca-5ca5-4ba0-9aeb-62a33cc2883a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ps -ef |grep text-generatio  | awk '{print $2}' | xargs kill -9\n",
    "!text-generation-launcher \\\n",
    "--quantize bitsandbytes --json-output \\\n",
    "--max-input-length 4095 --max-total-tokens 4096 --max-batch-prefill-tokens 4095 \\\n",
    "--model-id /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
    "--port 30005 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ecc84-2f40-4ee1-823b-c8b58107cb7e",
   "metadata": {},
   "source": [
    "## SINGULARITY IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cba3ae-bd8f-4e72-ad71-5cd6b388c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps -ef |grep text-generatio  | awk '{print $2}' | xargs kill -9\n",
    "!mkdir /tmp/$(whoami)\n",
    "!/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity exec --nv \\\n",
    "--no-home -B /tmp/$(whoami):/data -B /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf \\\n",
    "/work/u00cjz00/nvidia/text-generation-inference_1.1.1.sif \\\n",
    "text-generation-launcher --quantize bitsandbytes --json-output --sharded false --model-id /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf --port 30005 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e0a8c39-6bda-4d09-bb98-f16aa1125aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: sending signal to 226528 failed: No such process\n"
     ]
    }
   ],
   "source": [
    "!ps -ef |grep text-generatio  | awk '{print $2}' | xargs kill -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e105af0-5255-4f04-b954-f3da9b9335b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
