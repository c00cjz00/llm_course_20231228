{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6EMFg6fTWei"
   },
   "source": [
    "# MEDICAL CHATBOT\n",
    "CHAINLIT+PDF+FAISS_DB+LLAMA2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcxPV0Q4zPW-"
   },
   "source": [
    "## 初始環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJ-Om_qlzPW_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始環境設定\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y865t7qczPXA"
   },
   "source": [
    "## 確認CUDA版本, 以及否能使用GPU\n",
    "若無gpu 請點選右側->已連線->變更執行階段類型->T4 Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdlvsZZTzPXA",
    "outputId": "ae0acb7a-2c7a-4980-8935-65398d0e21aa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4oZaGRMzPXB"
   },
   "source": [
    "## 安裝套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctLWSpJYzPXB",
    "outputId": "c2e3c916-96ed-40eb-b86a-0c4ba759c61b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install cohere gdown==4.7.3 kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q\n",
    "!pip install accelerate bitsandbytes chainlit==0.7.700 faiss-cpu hf_transfer huggingface_hub optimum transformers -q\n",
    "!pip install auto-gptq -q\n",
    "#!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  -q # Use cu117 if on CUDA 11.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u69m9gq40x-_"
   },
   "source": [
    "### 文件下載"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J8QjjxAyz5GJ",
    "outputId": "5a277abb-5adc-455f-e1c6-08f646ec5f62",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p data/book/\n",
    "!gdown 1pUDgs3YMnlr8See8Rld3L1ZRiTeeOlMM -O data/book/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnqjWAd3zPXB"
   },
   "source": [
    "## Step1: create ingest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNADmOT9zPXB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << \\EOF >  ingest.py\n",
    "\n",
    "# RUN: python3 ingest.py\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "DATA_PATH = 'data/book/'\n",
    "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "\n",
    "# Create vector database\n",
    "def create_vector_db():\n",
    "    loader = DirectoryLoader(DATA_PATH,\n",
    "                             glob='*.pdf',\n",
    "                             loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                                   chunk_overlap=50)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "\n",
    "    db = FAISS.from_documents(texts, embeddings)\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_vector_db()\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36vYQbgk07Us"
   },
   "source": [
    "### 資料入庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQ4Jv_z2hRh6",
    "outputId": "b023ccbc-62d2-419b-d0b0-5900f57bca18",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 資料入庫 to FAISS\n",
    "!python3 ingest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHRUku5xzPXC",
    "tags": []
   },
   "source": [
    "## Step2: create model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjLyW3EGzPXC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << \\EOF >  model.py\n",
    "# RUN: chainlit run model.py\n",
    "\n",
    "# 01: CONFIGURE MODEL_ID and DB PATHs\n",
    "MODEL_ID = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "\n",
    "# 02: Load LIBRARY\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "import chainlit as cl\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import os\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "# 03: custom_prompt_template\n",
    "custom_prompt_template3 = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "custom_prompt_template=\"\"\"<|im_start|>system\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Question: {question}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt():\n",
    "    \"\"\"\n",
    "    Prompt template for QA retrieval for each vectorstore\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n",
    "    return prompt\n",
    "\n",
    "# 04: Retrieval QA Chain\n",
    "def retrieval_qa_chain(llm, prompt, db):\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=db.as_retriever(search_kwargs={'k': 5}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt': prompt}\n",
    "                                       )\n",
    "    return qa_chain\n",
    "\n",
    "def load_llm():\n",
    "    # 04: LLM模型 GPTQ\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "    llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.7})\n",
    "\n",
    "    return llm\n",
    "\n",
    "# 05: QA Model Function\n",
    "def qa_bot():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n",
    "    db = FAISS.load_local(DB_FAISS_PATH, embeddings)\n",
    "    llm = load_llm()\n",
    "    qa_prompt = set_custom_prompt()\n",
    "    qa = retrieval_qa_chain(llm, qa_prompt, db)\n",
    "\n",
    "    return qa\n",
    "\n",
    "# 06: output function\n",
    "def final_result(query):\n",
    "    qa_result = qa_bot()\n",
    "    response = qa_result({'query': query})\n",
    "    return response\n",
    "\n",
    "# 07: chainlit code\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    chain = qa_bot()\n",
    "    msg = cl.Message(content=\"Starting the bot...\")\n",
    "    await msg.send()\n",
    "    msg.content = \"Hi, Welcome to Medical Bot. What is your query?\"\n",
    "    await msg.update()\n",
    "\n",
    "    cl.user_session.set(\"chain\", chain)\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    chain = cl.user_session.get(\"chain\")\n",
    "    cb = cl.AsyncLangchainCallbackHandler(\n",
    "        stream_final_answer=True, answer_prefix_tokens=[\"FINAL\", \"ANSWER\"]\n",
    "    )\n",
    "    cb.answer_reached = True\n",
    "    res = await chain.acall(message.content, callbacks=[cb])\n",
    "    answer = res[\"result\"]\n",
    "    sources = res[\"source_documents\"]\n",
    "    source_elements = []\n",
    "\n",
    "    found_sources = []\n",
    "\n",
    "    for i, doc in enumerate(sources):\n",
    "        page_content=(res['source_documents'][i].page_content)\n",
    "        page=(res['source_documents'][i].metadata[\"page\"])\n",
    "        source=res['source_documents'][i].metadata[\"source\"]\n",
    "        file = os.path.basename(source)\n",
    "        #print(\"SOURCE: \"+file+\", PAGE: \"+str(page) )\n",
    "        source_name=f\"source_{i}\"\n",
    "\n",
    "        found_sources.append(source_name)\n",
    "        text=\"[Document: \"+file+\", Page: \"+str(page)+\"]\\n\\nContent: \"+page_content\n",
    "        source_elements.append(cl.Text(content=text, name=source_name))\n",
    "\n",
    "\n",
    "\n",
    "    if sources:\n",
    "        #answer += f\"\\n\\n\\n資料來源, Sources:\" + str(sources)\n",
    "        answer += f\"\\nSources: {', '.join(found_sources)}\"\n",
    "    else:\n",
    "        answer += \"\\n\\n\\nNo sources found\"\n",
    "\n",
    "    #await cl.Message(content=answer).send()\n",
    "    await cl.Message(content=answer, elements=source_elements).send()\n",
    "\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YO0zjj-SzPXD"
   },
   "source": [
    "## 執行模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNhQSeFOzPXD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CHAINLIT\n",
    "!chainlit run model.py -w &> /content/logs.txt &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qd7LoHR752rg",
    "outputId": "2ed2e870-e4c0-4aac-de04-de74410edf90"
   },
   "outputs": [],
   "source": [
    "!ngrok config add-authtoken xxxxxxxxxxxxxxxx\n",
    "\n",
    "from pyngrok import ngrok\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print('Public URL:', ngrok_tunnel.public_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHoEEHrbzPXE",
    "tags": []
   },
   "source": [
    "## DELETE JOB, 結束前再執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrok.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfFTVIWKkhgt"
   },
   "outputs": [],
   "source": [
    "!ps -ef |grep chainlit | awk '{print $2}' | xargs kill -9\n",
    "!ps -ef |grep ngrok | awk '{print $2}' | xargs kill -9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tk3Ol-XUzPXE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_S_work-genai3_pytorch_2.1.0-cuda11.8-cudnn8-runtime_textgen",
   "language": "python",
   "name": "s_work-genai3_pytorch_2.1.0-cuda11.8-cudnn8-runtime_textgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
