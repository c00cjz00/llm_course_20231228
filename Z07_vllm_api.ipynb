{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5070051-c2bb-4f65-a4a5-b9fa10a522b5",
   "metadata": {},
   "source": [
    "## vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024b38d9-fb5d-499c-8748-0a95c9b99032",
   "metadata": {},
   "source": [
    "## 初始環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e82eeeb-58da-434a-83c2-9461b6cdc32b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/g00cjz00/github/Llama2-Chinese/inference-speed/GPU/vllm_example'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079679b-522b-49b1-a092-703ffae8daec",
   "metadata": {},
   "source": [
    "## 安裝套件\n",
    "安裝完成後建議, 點選上方選單, 直接階段->重新啟動工作階段, 確保 library重置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaeb2c11-f903-4546-8b05-e7b3bfead5d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/tmp/user-pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: The directory '/tmp/user-pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/vllm-project/vllm/releases/download/v0.3.0/vllm-0.3.0+cu118-cp310-cp310-manylinux1_x86_64.whl -q\n",
    "!pip install --upgrade xformers==0.0.23.post1 torch==2.1.2 --index-url https://download.pytorch.org/whl/cu118 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f160880-037a-4adc-99af-19e27a31cafc",
   "metadata": {},
   "source": [
    "## 確認CUDA版本, 以及否能使用GPU\n",
    "若無gpu 請點選右側->已連線->變更執行階段類型->T4 Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62b55201-6099-443c-9dd1-8091e5c3bc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 11 17:06:07 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    41W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    42W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "2.1.2+cu118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(torch.__version__)  #注意是双下划线\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568d017-96b2-4669-aedf-319f78dad160",
   "metadata": {},
   "source": [
    "## MEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a611fec-fdbf-478d-bd67-3c8db7eae6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.executable=\"/usr/bin/python3.10\"\n",
    "#!{sys.executable} -V\n",
    "\n",
    "#from pprint import pprint\n",
    "#pprint(sys.path)\n",
    "#sys.path.append(’需要引用模块的地址')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35de45-97dd-44be-8774-04aabd08d78c",
   "metadata": {},
   "source": [
    "###　Model Vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6b692-99aa-41af-9d08-105f2e08f5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m vllm.entrypoints.api_server \\\n",
    "    --model /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf --swap-space 16 \\\n",
    "    --disable-log-requests --host 0.0.0.0 --port 8080 --max-num-seqs 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25b7e5-b0dd-4159-84dd-2f8ee0e6bd84",
   "metadata": {},
   "source": [
    "##　Model Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05100f5d-531c-4947-afc6-b3c4cd613cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m vllm.entrypoints.openai.api_server \\\n",
    "--model /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf --swap-space 16 \\\n",
    "--disable-log-requests --host 0.0.0.0 --port 5001 --max-num-seqs 20 --served-model-name llama-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d93804d-7168-4f78-918c-98c19cec4929",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3d60d-0362-469e-8ebd-2b416da8d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://gpn3001:5001/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6590b-022c-4e86-93a6-f77a0f3d5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl http://gpn3001:5001/v1/completions \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "\"model\": \"llama-2\",\n",
    "\"prompt\": \"San Francisco is a\",\n",
    "\"max_tokens\": 70,\n",
    "\"temperature\": 0\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17013510-5015-4772-bc5a-69c6b176be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl http://gpn3001:5001/v1/chat/completions \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "\"model\": \"llama-2\",\n",
    "\"messages\": [\n",
    "{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "{\"role\": \"user\", \"content\": \"什麼是聯邦式學習?\"}\n",
    "]\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9d0e1-5cd1-4452-9556-1a50fa0f42d0",
   "metadata": {},
   "source": [
    "## Singularity　Model Openai\n",
    "change kernel to Pytohn 3 (ipykernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98261ae-73b6-4c99-b5a7-c0677bd6fd0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-11 17:12:12 api_server.py:209] args: Namespace(host='0.0.0.0', port=5001, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name='llama-2', chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, root_path=None, middleware=[], model='/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf', tokenizer=None, revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, block_size=16, seed=0, swap_space=16, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=20, max_paddings=256, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, engine_use_ray=False, disable_log_requests=True, max_log_len=None)\n",
      "INFO 02-11 17:12:12 llm_engine.py:72] Initializing an LLM engine with config: model='/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf', tokenizer='/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, seed=0)\n",
      "INFO 02-11 17:12:23 llm_engine.py:322] # GPU blocks: 1855, # CPU blocks: 2048\n",
      "INFO 02-11 17:12:29 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-11 17:12:29 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-11 17:12:30 model_runner.py:698] Graph capturing finished in 1 secs.\n",
      "WARNING 02-11 17:12:30 serving_chat.py:264] No chat template provided. Chat API will not work.\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m13\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:5001\u001b[0m (Press CTRL+C to quit)\n",
      "INFO 02-11 17:12:40 llm_engine.py:877] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-11 17:12:50 llm_engine.py:877] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-11 17:13:00 llm_engine.py:877] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-11 17:13:10 llm_engine.py:877] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
      "INFO 02-11 17:13:20 llm_engine.py:877] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "!/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity exec --nv -C \\\n",
    "-B /work -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin \\\n",
    "-B /work/g00cjz00/libraryFolder/P-3.10-work-genai_cuda_11.8.0-cudnn8-runtime-ubuntu22.04/local/lib:/home/g00cjz00/.local/lib \\\n",
    "-B /work/g00cjz00/libraryFolder/P-3.10-work-genai_cuda_11.8.0-cudnn8-runtime-ubuntu22.04/local/bin:/home/g00cjz00/.local/bin \\\n",
    "/work/u00cjz00/nvidia/cuda_11.8.0-cudnn8-runtime-ubuntu22.04.sif \\\n",
    "bash -c 'export PATH=$PATH:$HOME/.local/bin; \\\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\n",
    "--model /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf --swap-space 16 \\\n",
    "--disable-log-requests --host 0.0.0.0 --port 5001 --max-num-seqs 20 --served-model-name llama-2'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
