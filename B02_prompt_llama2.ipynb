{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507788fa-cb65-4c26-a3b6-b933f8d82d4d",
   "metadata": {},
   "source": [
    "## Langchain PROMPT -  Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706cbb5-17e6-4a3f-aac6-d3a0903acf40",
   "metadata": {},
   "source": [
    "1. Llama-2-7b-chat-hf DEMO\n",
    "- https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "2. LANGCHAIN 手冊\n",
    "https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/\n",
    "\n",
    "3. 學習LANGCHAIN -> llm +  prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d99bb-091b-4607-92d1-d4aa24a89c94",
   "metadata": {},
   "source": [
    "## 初始環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c755e8a-6056-4abf-9931-8d6d4e63b127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ea82f-e7da-4b8a-99cc-3c2097f61363",
   "metadata": {},
   "source": [
    "## 確認CUDA版本, 以及否能使用GPU\n",
    "若無gpu 請點選右側->已連線->變更執行階段類型->T4 Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46218ad7-ab3f-476d-bc93-885f4d65cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145ec44-0b71-41bd-92c0-fefd9affb806",
   "metadata": {},
   "source": [
    "## 安裝套件\n",
    "安裝完成後建議, 點選上方選單, 直接階段->重新啟動工作階段, 確保 library重置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26741fe-c66f-407b-ac81-959529bb7b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install cohere gdown kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q\n",
    "!pip install accelerate bitsandbytes hf_transfer huggingface_hub optimum transformers -q "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce2ca1-96b4-4635-a670-d069562cc5ed",
   "metadata": {},
   "source": [
    "### HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562f562-502e-430d-97f2-1ba19cbc6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF_TOKEN method 1\n",
    "\n",
    "!echo \"HF_TOKEN=hf_xxxxxxx\" > .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # loads env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7044a4-6bb2-40df-8756-52d12fef74b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HF_TOKEN method 2\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_xxxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6718f-fd01-4b0f-85a7-c008978bc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAPI KEY  method 3\n",
    "\n",
    "import os\n",
    "from typing import TextIO\n",
    "from getpass import getpass\n",
    "os.environ[\"HF_TOKEN\"] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131aa253-8cb7-4171-9f32-7923fc3384ea",
   "metadata": {},
   "source": [
    "### LOAD LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261cc729-b6b3-43a8-9d69-2620c9d4016d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOAD LIBRARY\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561e895-1187-405a-92e9-6c2d0eb5d9a8",
   "metadata": {},
   "source": [
    "### Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9fdb4-569c-422c-8c04-bef242980493",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download model\n",
    "mkdir -p /content/Llama-2-7b-chat-hf\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir /content/Llama-2-7b-chat-hf  --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b644634-295d-4baf-9d66-a1d11fdb3edd",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "temperature 的參數值越小，模型就會回傳越確定的結果。如果調高該參數值，大語言模型可能會返回更隨機的結果，也就是說這可能會帶來更多樣化或更具創造性的產出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8bb11-126d-4243-a4b3-f13e3caaf7ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "MODEL_ID = \"/content/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# Pipeline\n",
    "#################################################################\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5eca6-b96f-4c1d-a7f7-89a99fecbb7e",
   "metadata": {},
   "source": [
    "### QUESTION to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2a2878-965f-4af9-becd-23674f33566d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response=llm(\"什麼是聯邦式學習?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e01bf0-883d-49ee-a24a-529bf93fd017",
   "metadata": {},
   "source": [
    "## LANGCHAIN PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186af28b-86f2-43b5-a6ba-79bb277ac665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT01\n",
    "template01=\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{question}[/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "template02=\"\"\"<|im_start|>system\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<|im_end|>\n",
    "<|im_start|>user\n",
    "{question}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "template03=\"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "\n",
    "### Instruction:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt01 = PromptTemplate(template=template01, input_variables=[\"question\"])\n",
    "prompt02 = PromptTemplate(template=template02, input_variables=[\"question\"])\n",
    "prompt03 = PromptTemplate(template=template03, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8c2b4-1604-4f28-ace9-25f6690cb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT RESULT\n",
    "question = \"什麼是聯邦式學習?\"\n",
    "print(prompt01.format(question=question))\n",
    "\n",
    "print(prompt02.format(question=question))\n",
    "\n",
    "print(prompt03.format(question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ddeeeb-6a73-480e-9726-032ec9b3b56a",
   "metadata": {},
   "source": [
    "## LANGCHAIN LLM+PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bdfdb3-8714-44e8-84da-1decae6e093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Chain (prompt + model)\n",
    "chain01 = LLMChain(llm=llm, prompt=prompt01)\n",
    "\n",
    "chain02 = LLMChain(llm=llm, prompt=prompt02)\n",
    "\n",
    "chain03 = LLMChain(llm=llm, prompt=prompt03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74036f2-75bf-4dc6-b94f-f9941b19d371",
   "metadata": {},
   "source": [
    "### Lnagchain QUESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1653b-938d-41fe-9a63-ec6a7436e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"什麼是聯邦式學習?\"\n",
    "print(chain01.run(question=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43cd80-371a-4251-8963-26b94980ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"什麼是聯邦式學習?\"\n",
    "print(chain02.run(question=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e803bfe-1ab3-4f3a-8b0b-19e17df6e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"什麼是聯邦式學習?\"\n",
    "print(chain03.run(question=question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
