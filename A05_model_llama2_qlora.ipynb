{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507788fa-cb65-4c26-a3b6-b933f8d82d4d",
   "metadata": {},
   "source": [
    "## Langchain LLM - Llama-2-7b-chat-hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706cbb5-17e6-4a3f-aac6-d3a0903acf40",
   "metadata": {},
   "source": [
    "1. Llama-2-7b-chat-hf DEMO\n",
    "- https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
    "\n",
    "2. LANGCHAIN 手冊\n",
    "https://python.langchain.com/docs/integrations/llms/openai\n",
    "\n",
    "3. 學習llm 模組, 如 llm(\"什麼是聯邦式學習?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d99bb-091b-4607-92d1-d4aa24a89c94",
   "metadata": {},
   "source": [
    "## 初始環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c755e8a-6056-4abf-9931-8d6d4e63b127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6a072-8c6c-4498-94aa-2a11321d71d9",
   "metadata": {},
   "source": [
    "## 確認CUDA版本, 以及否能使用GPU\n",
    "若無gpu 請點選右側->已連線->變更執行階段類型->T4 Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9426b-a114-4f33-ae5a-eeb09a95a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145ec44-0b71-41bd-92c0-fefd9affb806",
   "metadata": {},
   "source": [
    "## 安裝套件\n",
    "安裝完成後建議, 點選上方選單, 直接階段->重新啟動工作階段, 確保 library重置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912186d1-3525-409b-bcb2-61863c415434",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cohere gdown kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q\n",
    "!pip install accelerate bitsandbytes hf_transfer huggingface_hub optimum transformers -q \n",
    "!pip install fire -q\n",
    "!pip install git+https://github.com/huggingface/peft.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01390959-2a9d-4203-a2cc-7c3a689e54fd",
   "metadata": {},
   "source": [
    "### HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bd6ce-37d9-4cc6-8124-96ccf3c28285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF_TOKEN method 1\n",
    "\n",
    "!echo \"HF_TOKEN=hf_xxxxxxxxxxx\" > .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # loads env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6089d28c-0f62-4ba9-895a-fd0958679df0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HF_TOKEN method 2\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_xxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3513274b-26a0-4414-9127-79a57f4367ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OPENAPI KEY  method 3\n",
    "\n",
    "import os\n",
    "from typing import TextIO\n",
    "from getpass import getpass\n",
    "os.environ[\"HF_TOKEN\"] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131aa253-8cb7-4171-9f32-7923fc3384ea",
   "metadata": {},
   "source": [
    "### 方法一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261cc729-b6b3-43a8-9d69-2620c9d4016d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LOAD LIBRARY\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig, GenerationConfig\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "import torch\n",
    "from peft import PeftConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c767b5-16d8-4439-a12f-5f76b5625418",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b5dd16a9d04b948358055389a47a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "#Set Path to folder that contains adapter_config.json and the associated .bin files for the Peft model\n",
    "#peft_model_id = '/path/to/local/peft_model_folder\n",
    "MODEL_ID=\"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\"\n",
    "LORA_ID=\"/work/u00cjz00/slurm_jobs/github/loras/math\"\n",
    "\n",
    "#Get PeftConfig from the finetuned Peft Model. This config file contains the path to the base model\n",
    "config = PeftConfig.from_pretrained(LORA_ID)\n",
    "config.base_model_name_or_path=MODEL_ID\n",
    "\n",
    "# If you quantized the model while finetuning using bits and bytes \n",
    "# and want to load the model in 4bit for inference use the following code.\n",
    "# NOTE: Make sure the quant and compute types match what you did during finetuning\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "\n",
    ")\n",
    "\n",
    "# Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    #use_auth_token=True,    \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the Peft/Lora model\n",
    "model = PeftModel.from_pretrained(model, LORA_ID)\n",
    "\n",
    "\n",
    "# unwind broken decapoda-research config (可以試試看取消看看差異)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "# Pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    do_sample=True,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "# langchain llm\n",
    "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5eca6-b96f-4c1d-a7f7-89a99fecbb7e",
   "metadata": {},
   "source": [
    "### QUESTION to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2a2878-965f-4af9-becd-23674f33566d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Unterscheidung zwischen einem Satz und einer Aussage. \n",
      "- 示例：\n",
      "小明有6顆糖果，他把其中的3顆糖果送給了小紅。那麼小明現在手中還剩下幾顆糖果？\n",
      "答案：小明現在手中還剩下3顆糖果。\n",
      "根據題目可知：\n",
      "小明有6顆糖果\n",
      "小明把其中的3顆糖果送給了小紅\n",
      "因此，小明現在手中還剩下的糖果數量等於6-3=3顆。所以，答案是3顆。\n",
      "因此，我們需要用到加法和減法來解決這道問題。\n",
      "2. 理解數學符號的意義。\n",
      "- 示例：\n",
      "小明有6顆糖果，他把其中的3顆糖果送給了小紅，那麼小明現在手中還剩下幾顆糖果？\n",
      "答案：小明現在手中還剩下3顆糖果。\n",
      "根據題目可知：\n",
      "小明有6顆糖果\n",
      "小明把其中的3顆糖果送給了小紅\n",
      "因此，小明現在手中還剩下的糖果數量等於6-3=3顆。所以，答案是3顆。\n",
      "因此，我們需要用到加法和減法來解決這道�\n"
     ]
    }
   ],
   "source": [
    "response=llm(\"小明有14顆糖果，他送給小紅5顆，還給小王4顆，請問他現在手中還剩幾顆糖果?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4a8d2-569c-4132-8db3-2acf5519d803",
   "metadata": {},
   "source": [
    "## 方法二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707db3a9-fad9-4328-b44e-e3d9cdce67da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers.utils import is_accelerate_available, is_bitsandbytes_available\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "ALPACA_TEMPLATE = (\n",
    "    \"Below is an instruction that describes a task, paired with an input that provides \"\n",
    "    \"further context. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def load_adapted_hf_generation_pipeline(\n",
    "    base_model_name,\n",
    "    lora_model_name,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 1.,\n",
    "    max_tokens: int = 512,\n",
    "    batch_size: int = 16,\n",
    "    device: str = \"cuda\",\n",
    "    load_in_8bit: bool = True,\n",
    "    generation_kwargs: Optional[dict] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a huggingface model & adapt with PEFT.\n",
    "    Borrowed from https://github.com/tloen/alpaca-lora/blob/main/generate.py\n",
    "    \"\"\"\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        if not is_accelerate_available():\n",
    "            raise ValueError(\"Install `accelerate`\")\n",
    "    if load_in_8bit and not is_bitsandbytes_available():\n",
    "            raise ValueError(\"Install `bitsandbytes`\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    task = \"text-generation\"\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            load_in_8bit=load_in_8bit,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    elif device == \"mps\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name, device_map={\"\": device}, low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_model_name,\n",
    "            device_map={\"\": device},\n",
    "        )\n",
    "\n",
    "    # unwind broken decapoda-research config\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    model.config.bos_token_id = 1\n",
    "    model.config.eos_token_id = 2\n",
    "\n",
    "    if not load_in_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generation_kwargs = generation_kwargs if generation_kwargs is not None else {}\n",
    "    config = GenerationConfig(\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        **generation_kwargs,\n",
    "    )\n",
    "    pipe = pipeline(\n",
    "        task,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=16, # TODO: make a parameter\n",
    "        generation_config=config,\n",
    "        framework=\"pt\",\n",
    "    )\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d232a6-9e6d-4a70-94af-15dc94b3e903",
   "metadata": {},
   "source": [
    "### QUESTION to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f53b1f-5fe0-415e-8af9-57d80bf29b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = load_adapted_hf_generation_pipeline(\n",
    "    base_model_name=\"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\",\n",
    "    lora_model_name=\"/work/u00cjz00/slurm_jobs/github/loras/math\"\n",
    ")\n",
    "prompt_template = ALPACA_TEMPLATE.format(\n",
    "    instruction=\"請回答以下數學問題\",\n",
    "    input=\"小明有14顆糖果，他送給小紅5顆，還給小王4顆，請問他現在手中還剩幾顆糖果?\"\n",
    ")\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c48d7-2565-4115-97e0-2776150be5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Image_S_work-genai2_pytorch_2.1.0-cuda11.8-cudnn8-runtime_textgen",
   "language": "python",
   "name": "s_work-genai2_pytorch_2.1.0-cuda11.8-cudnn8-runtime_textgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
