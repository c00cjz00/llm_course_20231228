{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d0ea98-07c8-4577-90d5-a3baaf80c7e1",
   "metadata": {},
   "source": [
    "## Langchain Chain - OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5849f6-959b-417e-a20e-79816a0e7782",
   "metadata": {},
   "source": [
    "1. OPENAI\n",
    "查看是否有額度\n",
    "- https://platform.openai.com/account/billing/overview\n",
    "- https://platform.openai.com/usage\n",
    "\n",
    "2. LANGCHAIN 手冊\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\n",
    "3. 學習LANGCHAIN -> CHAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bad7c7-2622-4b99-8189-a2bbb8550ca5",
   "metadata": {},
   "source": [
    "## 初始環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10aad2a-c43d-4203-bd42-6ab6936700c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5a31d7-299e-46ee-831e-08a46b11a950",
   "metadata": {},
   "source": [
    "## 確認CUDA版本, 以及否能使用GPU\n",
    "若無gpu 請點選右側->已連線->變更執行階段類型->T4 Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf53aa3-61be-45d2-adbc-f2c0d9cefa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847bbd94-c432-4f4f-a0ac-86a2e9916b6d",
   "metadata": {},
   "source": [
    "## 安裝套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b984f-4b5d-4a94-a30e-b97dff0e5f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## For colab\n",
    "!pip install chromadb cohere gdown kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c2f24-eaaf-456c-bd31-0a7bb349649f",
   "metadata": {},
   "source": [
    "### OPENAI API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c61e65-4bf2-4a02-9104-13d3ed083909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OPENAPI KEY method 1\n",
    "\n",
    "!echo \"OPENAI_API_KEY=sk-xxxxxxxxxxxxx\" > .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() # loads env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a41f1-ce18-4e91-b66b-e366ce9b5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAPI KEY  method 2\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxxxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da186f-9f7f-41ed-8d23-19c1d85dcd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAPI KEY  method 3\n",
    "\n",
    "import os\n",
    "from typing import TextIO\n",
    "from getpass import getpass\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654b8d7-606a-4ccc-ae1a-1be0d70f1e62",
   "metadata": {},
   "source": [
    "### 1. LLMChain (llm + prompt)\n",
    "LLMChain is most basic chain in Langchai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f629f-b9f8-4c1d-8fbf-c811d882a8e8",
   "metadata": {
    "id": "366f629f-b9f8-4c1d-8fbf-c811d882a8e8",
    "language": "python",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Library\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# LLM Model\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"what are the 5 most {input} cities in the world?\",\n",
    ")\n",
    "\n",
    "# Chain\n",
    "chain=LLMChain(llm=llm,prompt=prompt)\n",
    "\n",
    "# RUN Chain\n",
    "result=chain.run('populated')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7904f58-aa9b-453f-83ad-b4fd61505ec4",
   "metadata": {},
   "source": [
    "### ^^ 練習, 更換 prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5e6b5-5a7c-470b-9e75-a4a8ccdc3a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 練習將 prompt 更換為 prompt01 and prompt02\n",
    "prompt01 = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Give me a tweet idea on {topic}?\",\n",
    ")\n",
    "\n",
    "prompt02 = PromptTemplate(\n",
    "    input_variables=[\"topic1\", \"topic2\"],\n",
    "    template=\"Give me a tweet idea on {topic1} and {topic2}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a302fec-2ae5-4c25-b86c-3bb01bd1853c",
   "metadata": {},
   "source": [
    "## 2. Sequential Chain  (llm + prompt)\n",
    "A sequential chain works by combining two or more chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1df1e7-b3fe-4765-9d25-0fb0bdf35bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load library\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# LLM Model\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# 1. TOPIC Template\n",
    "template = \"\"\"Write a blog outline given a topic.\n",
    "\n",
    "Topic: {topic}\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
    "outline_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"outline\")\n",
    "print(prompt.format(topic=\"寫出台南旅遊規劃大綱\"))\n",
    "\n",
    "\n",
    "# 2. Outline Template\n",
    "template = \"\"\"Write a blog article based on the below outline.\n",
    "\n",
    "Outline:\n",
    "{outline}\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"outline\"], template=template)\n",
    "article_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"article\")\n",
    "print(prompt.format(outline=\"從旅遊規劃大綱, 寫出台南旅遊遊記\"))\n",
    "\n",
    "# Sequential Chain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[outline_chain, article_chain],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"outline\", \"article\"],\n",
    "    verbose=True)\n",
    "\n",
    "# Chain Run\n",
    "result=overall_chain({\"topic\":\"台南旅遊規劃\"})\n",
    "\n",
    "# Result\n",
    "print(result[\"topic\"])\n",
    "print(result[\"outline\"])\n",
    "print(result[\"article\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf215bb0-3fd2-4460-838a-80df587316e5",
   "metadata": {},
   "source": [
    "## 3. Retrieval QA chain  (model + prompt + documents + vectordb)\n",
    "\n",
    "Retrieval QA chain is considered one of the most important helping with doing QA over your document data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c58cf3-c463-43c3-9940-149a7490c9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p data/pdf/\n",
    "!gdown 1AldhEWVCtcE50XARgSnXR0azZ965nNmT -O data/pdf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c638c3d0-8b93-4a45-8f9f-dede4f2f8feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load library\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 文件解析\n",
    "pdf_file='./data/pdf/e2729e76-29a0-4be5-9eef-67809b05d6b9.pdf'\n",
    "loader= PyPDFLoader(pdf_file)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 向量資料庫\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectortdb = Chroma.from_documents(texts, embeddings)\n",
    "#DB_PATH = 'vectorstore/db_chroma'\n",
    "#vectortdb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=DB_PATH)\n",
    "\n",
    "# Load DB\n",
    "#embeddings = OpenAIEmbeddings()\n",
    "#DB_PATH = 'vectorstore/db_chroma'\n",
    "#vectortdb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)\n",
    "\n",
    "#: Test Search in Vector DB\n",
    "query = \"請說明櫃公司如何進行資產管理?\"\n",
    "source_documents=vectortdb.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(source_documents):\n",
    "    page_content=source_documents[i].page_content\n",
    "    page=source_documents[i].metadata[\"page\"]\n",
    "    source=source_documents[i].metadata[\"source\"]\n",
    "    file = os.path.basename(source) \n",
    "    print(\"Source: \"+file+\", Page \"+str(page+1) )\n",
    "    print(page_content)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08809294-9c39-4a87-ba0b-01547a84f2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  RetrievalQA Chain 搜尋\n",
    "#llm=OpenAI(temperature=0.7)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k-0613\", temperature=0, streaming=True)\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectortdb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Search\n",
    "query = \"請說明櫃公司如何進行資產管理?\"\n",
    "llm_response = chain(query)\n",
    "print(llm_response['query'])\n",
    "print(llm_response['result'])\n",
    "print(llm_response['source_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5474a-4f75-447a-b094-dc100c3665fd",
   "metadata": {},
   "source": [
    "## 4. RetrievalQAWithSourcesChain  (model + prompt + documents + vectordb)\n",
    "Retrieval QA chain is considered one of the most important helping with doing QA over your document data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da212c-3d30-4d1d-bfc3-8827e6fef9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load library\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 文件解析\n",
    "pdf_file='./data/pdf/e2729e76-29a0-4be5-9eef-67809b05d6b9.pdf'\n",
    "loader= PyPDFLoader(pdf_file)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 向量資料庫\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectortdb = Chroma.from_documents(texts, embeddings)\n",
    "#DB_PATH = 'vectorstore/db_chroma'\n",
    "#vectortdb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=DB_PATH)\n",
    "\n",
    "# Load DB\n",
    "#embeddings = OpenAIEmbeddings()\n",
    "#DB_PATH = 'vectorstore/db_chroma'\n",
    "#vectortdb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)\n",
    "\n",
    "#: Test Search in Vector DB\n",
    "query = \"請說明櫃公司如何進行資產管理?\"\n",
    "source_documents=vectortdb.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(source_documents):\n",
    "    page_content=source_documents[i].page_content\n",
    "    page=source_documents[i].metadata[\"page\"]\n",
    "    source=source_documents[i].metadata[\"source\"]\n",
    "    file = os.path.basename(source) \n",
    "    print(\"Source: \"+file+\", Page \"+str(page+1) )\n",
    "    print(page_content)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044dfeae-58e0-48ec-aed1-bff846985ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  RetrievalQAWithSourcesChain Chain 搜尋 + PROMPT\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "template = '''\n",
    "Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\").\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "\n",
    "{summaries}\n",
    "\n",
    "Respond in the persona of 財務專家\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"summaries \",\"question\"], template=template)\n",
    "\n",
    "# Initialise RetrievalQA Chain\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    #llm=OpenAI(temperature=0.7),\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k-0613\", temperature=0, streaming=True),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectortdb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Search\n",
    "query = \"請說明櫃公司如何進行資產管理?\"\n",
    "llm_response = chain(query)\n",
    "print(llm_response['question'])\n",
    "print(llm_response['answer'])\n",
    "print(llm_response['sources'])\n",
    "print(llm_response['source_documents'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0e336-9ff7-46be-bbfb-7c8af7fab6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RetrievalQAWithSourcesChain 頁碼解析\n",
    "import os\n",
    "source_documents=llm_response['source_documents'];\n",
    "\n",
    "for i, doc in enumerate(source_documents):\n",
    "    page_content=(llm_response['source_documents'][i].page_content)\n",
    "    page=(llm_response['source_documents'][i].metadata[\"page\"])\n",
    "    source=llm_response['source_documents'][i].metadata[\"source\"]\n",
    "    file = os.path.basename(source) \n",
    "    print(\"SOURCE: \"+file+\", PAGE: \"+str(page) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cf49bf-0819-494d-ad6c-14c4bbeaa029",
   "metadata": {},
   "source": [
    "## 5. Create Memory Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5cbbe-7651-462f-af20-a7387890c6e0",
   "metadata": {
    "id": "bec5cbbe-7651-462f-af20-a7387890c6e0",
    "language": "python",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import ConversationChain, OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff66c31-dee5-4ec7-a3b8-07b448ebe69c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dff66c31-dee5-4ec7-a3b8-07b448ebe69c",
    "language": "python",
    "outputId": "dab51cc7-e4af-48ca-e994-c0ad0c1e7db2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Customize the LLM template\n",
    "template = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "\n",
    "{history}\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
    "\n",
    "print(prompt.format(human_input=\"my_human_input\", history=\"my_history\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566dc258-9e1a-4724-9f9a-ce3a29e7dc96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "566dc258-9e1a-4724-9f9a-ce3a29e7dc96",
    "language": "python",
    "outputId": "241563f1-e429-4334-a8a0-96ca910e5b68",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create memory chain1\n",
    "chain = LLMChain(llm=llm,prompt=prompt,memory=ConversationBufferWindowMemory(k=2))\n",
    "\n",
    "# Predict a sentence using the chatgpt chain\n",
    "output = chain.run(human_input=\"請依序列出聯邦學習的重點\")\n",
    "\n",
    "# Display the model's response\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5788b0-e646-48ec-bf0c-dee3ac26ec43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa5788b0-e646-48ec-bf0c-dee3ac26ec43",
    "language": "python",
    "outputId": "2e54aedf-939e-48a9-f71a-5b769c85c750",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create memory chain2\n",
    "output = chain.run(human_input=\"請將以上的重點做一個結論\")\n",
    "\n",
    "# Display the model's response\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64c6e1-3d07-4e78-8be9-c6ccf8b35aa5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b64c6e1-3d07-4e78-8be9-c6ccf8b35aa5",
    "language": "python",
    "outputId": "a0fa7800-97b4-47bd-dd3d-0c3e146a6afe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create memory chain3\n",
    "output = chain.run(human_input=\"請將以上的總結, 規劃未來執行的方向\")\n",
    "\n",
    "# Display the model's response\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8327d-6355-454e-9452-f0b70eb9e1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupyterlab": {
   "notebooks": {
    "version_major": 6,
    "version_minor": 4
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "singlestore_cell_default_language": "python",
  "singlestore_connection": {
   "connectionID": "8bd78932-964d-4d20-91bc-72bfc2f211f6",
   "defaultDatabase": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
