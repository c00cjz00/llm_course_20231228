{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0PlUF-R2rmx"
   },
   "source": [
    "# oobabooga/text-generation-webui\n",
    "1. docker pull c00cjz00/pytorch:2.1.0-cuda11.8-cudnn8-runtime_textgen\n",
    "2. 01_create_image_ipykernel.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始環境設定\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate==0.27.2\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/g00cjz00/.local/lib/python3.10/site-packages (from accelerate==0.27.2) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/g00cjz00/.local/lib/python3.10/site-packages (from accelerate==0.27.2) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/g00cjz00/.local/lib/python3.10/site-packages (from accelerate==0.27.2) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/g00cjz00/.local/lib/python3.10/site-packages (from accelerate==0.27.2) (0.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/g00cjz00/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (4.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/g00cjz00/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.27.2) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/g00cjz00/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.27.2) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.27.2) (1.3.0)\n",
      "Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.25.0\n",
      "    Uninstalling accelerate-0.25.0:\n",
      "      Successfully uninstalled accelerate-0.25.0\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory and accelerate-launch are installed in '/home/g00cjz00/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed accelerate-0.27.2\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 28 13:56:37 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    54W / 300W |   2306MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    56W / 300W |   1866MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:B1:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    53W / 300W |   1866MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  Off  | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    56W / 300W |   2616MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     15949      C   python                           2303MiB |\n",
      "|    1   N/A  N/A     15949      C   python                           1863MiB |\n",
      "|    2   N/A  N/A     15949      C   python                           1863MiB |\n",
      "|    3   N/A  N/A     15949      C   python                           2613MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安裝 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!rm -rf /tmp/*\n",
    "!rm -rf text-generation-webui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下載程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete old version\n",
    "rm -rf text-generation-webui\n",
    "sleep 3\n",
    "#  DOWNLOAD text-generation-webui, install requirements.txt and execute this service \n",
    "REPOSRC=https://github.com/oobabooga/text-generation-webui.git\n",
    "LOCALREPO=text-generation-webui\n",
    "BRANCH=\"snapshot-2024-02-25\"\n",
    "BRANCH=\"snapshot-2023-12-10\"\n",
    "#BRANCH=\"snapshot-2023-12-17\"\n",
    "\n",
    "#BRANCH=\"snapshot-2024-01-07\"\n",
    "# We do it this way so that we can abstract if from just git later on\n",
    "LOCALREPO_VC_DIR=$LOCALREPO/.git\n",
    "\n",
    "if [ ! -d $LOCALREPO_VC_DIR ]\n",
    "then\n",
    "    git clone --branch $BRANCH $REPOSRC $LOCALREPO\n",
    "    #cp text-generation-webui/requirements.txt text-generation-webui/requirements.txt.BACKUP\n",
    "    #sed -i 's@cu121@cu118@g' text-generation-webui/requirements.txt\n",
    "    #sed -i 's@cu122@cu118@g' text-generation-webui/requirements.txt\n",
    "\n",
    "    # It does not need to pip with this IMAGE\n",
    "    #pip install -r text-generation-webui/requirements.txt -q \n",
    "    #pip install -r text-generation-webui/extensions/openai/requirements.txt -q      \n",
    "\n",
    "    # Prepare\n",
    "    cp ./textgen/config-user.yaml ./text-generation-webui/models/config-user.yaml\n",
    "    cp ./textgen/server_nchc.py ./text-generation-webui/server_nchc.py\n",
    "    cd ./text-generation-webui/models\n",
    "    ln -s /work/u00cjz00/slurm_jobs/github/models/Taiwan-LLM-7B-v2.1-chat-Q8_0.gguf .\n",
    "    ln -s /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf .\n",
    "    ln -s /work/u00cjz00/slurm_jobs/github/models/Llama-2-7B-Chat-GPTQ .\n",
    "    cd ../../\n",
    "    cd ./text-generation-webui/training/datasets\n",
    "    ln -s /work/u00cjz00/slurm_jobs/github/dataset/school_math_1000.json .\n",
    "    ln -s /work/u00cjz00/slurm_jobs/github/dataset/school_math_30000.json .\n",
    "    cd ../../../\n",
    "    cd ./text-generation-webui/loras\n",
    "    ln -s /work/u00cjz00/slurm_jobs/github/loras/math .\n",
    "    cd ../../ \n",
    "fi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp text-generation-webui/requirements.txt text-generation-webui/requirements.txt.BACKUP\n",
    "sed -i 's@cu121@cu118@g' text-generation-webui/requirements.txt\n",
    "sed -i 's@cu122@cu118@g' text-generation-webui/requirements.txt\n",
    "\n",
    "# It does not need to pip with this IMAGE\n",
    "pip install -r text-generation-webui/requirements.txt -U \n",
    "pip install -r text-generation-webui/extensions/openai/requirements.txt -U\n",
    "pip install hqq -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install hqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改程式碼 server.py\n",
    "Add -> root_path=shared.args.public_api_id\n",
    "```\n",
    "    # Launch the interface\n",
    "    shared.gradio['interface'].queue(concurrency_count=64)\n",
    "    with OpenMonkeyPatch():\n",
    "        shared.gradio['interface'].launch(\n",
    "            root_path=shared.args.public_api_id,    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install torch_grammar -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 啟動程式碼\n",
    "- 修改 password=\"nchc:nchcorgtw\" , 網頁帳號密碼\n",
    "- 修改 HF_TOKEN='hf_xxxxxxxx' , huggingface token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://node01.biobank.org.tw/rstudio/172.16.124.152/3000/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kill: (34428): No such process\n",
      "2024-02-28 14:00:33 INFO:\u001b[32mLoading the extension \"Training_PRO\"...\u001b[0m\n",
      "2024-02-28 14:00:33 INFO:\u001b[32mLoading the extension \"openai\"...\u001b[0m\n",
      "/home/g00cjz00/.local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_names\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "2024-02-28 14:00:33 INFO:\u001b[32mOpenAI-compatible API URL:\n",
      "\n",
      "http://0.0.0.0:9234\n",
      "\u001b[0m\n",
      "2024-02-28 14:00:33 INFO:\u001b[32mLoading the extension \"gallery\"...\u001b[0m\n",
      "INFO:     \u001b[32mStarted server process [34434]\u001b[0m\n",
      "INFO:     \u001b[32mWaiting for application startup.\u001b[0m\n",
      "INFO:     \u001b[32mApplication startup complete.\u001b[0m\n",
      "INFO:     \u001b[32mUvicorn running on http://0.0.0.0:9234 (Press CTRL+C to quit)\u001b[0m\n",
      "/home/g00cjz00/.local/lib/python3.10/site-packages/gradio/components/dropdown.py:231: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include: llama or set allow_custom_value=True.\n",
      "  warnings.warn(\n",
      "2024-02-28 14:01:10 INFO:\u001b[32mLoading Llama-2-7b-chat-hf...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://gpn3002:3000\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.20it/s]\n",
      "2024-02-28 14:01:37 INFO:\u001b[32mLOADER: Transformers\u001b[0m\n",
      "2024-02-28 14:01:37 INFO:\u001b[32mTRUNCATION LENGTH: 4096\u001b[0m\n",
      "2024-02-28 14:01:37 INFO:\u001b[32mINSTRUCTION TEMPLATE: Llama-v2\u001b[0m\n",
      "2024-02-28 14:01:37 INFO:\u001b[32mLoaded the model in 27.46 seconds.\u001b[0m\n",
      "2024-02-28 14:02:24 INFO:\u001b[32mLoading JSON datasets...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output generated in 9.62 seconds (1.35 tokens/s, 13 tokens, context 68, seed 65406446)\n",
      "Output generated in 1.51 seconds (18.55 tokens/s, 28 tokens, context 88, seed 943490799)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 69 examples [00:00, 753.07 examples/s]\n",
      "Map: 100%|██████████| 69/69 [00:00<00:00, 2384.13 examples/s]\n",
      "2024-02-28 14:02:25 INFO:\u001b[32mGetting model ready...\u001b[0m\n",
      "2024-02-28 14:02:26 INFO:\u001b[32mPreparing for training...\u001b[0m\n",
      "2024-02-28 14:02:26 INFO:\u001b[32mCreating LoRA model...\u001b[0m\n",
      "2024-02-28 14:02:26 WARNING:\u001b[33mDetected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-02-28 14:02:32 INFO:\u001b[32mStarting training...\u001b[0m\n",
      "2024-02-28 14:02:32 INFO:\u001b[32mLog file 'train_dataset_sample.json' created in the 'logs' directory.\u001b[0m\n",
      "Exception in thread Thread-7 (threaded_run):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/g00cjz00/demo2024/llm_course_20231228/text-generation-webui/modules/training.py\", line 696, in threaded_run\n",
      "    trainer.train()\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1624, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1757, in _inner_training_loop\n",
      "    model = self._wrap_model(self.model_wrapped)\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1376, in _wrap_model\n",
      "    model = self.ipex_optimize_model(model, training, dtype=dtype)\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 1352, in ipex_optimize_model\n",
      "    raise ImportError(\n",
      "ImportError: Using IPEX but IPEX is not installed or IPEX's version does not match current PyTorch, please refer to https://github.com/intel/intel-extension-for-pytorch.\n",
      "2024-02-28 14:02:32 INFO:\u001b[32mTraining complete, saving...\u001b[0m\n",
      "2024-02-28 14:02:32 INFO:\u001b[32mTraining complete!\u001b[0m\n",
      "2024-02-28 14:02:54 INFO:\u001b[32mLoading Llama-2-7b-chat-hf...\u001b[0m\n",
      "2024-02-28 14:02:54 INFO:\u001b[32mUsing the following 4-bit params: {'load_in_4bit': True, 'bnb_4bit_compute_dtype': torch.float16, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': False}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 'llama' model using (q, v) projections\n",
      "Trainable params: 16,777,216 (0.2484 %), All params: 6,755,192,832 (Model: 6,738,415,616)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.15s/it]\n",
      "2024-02-28 14:03:07 INFO:\u001b[32mLOADER: Transformers\u001b[0m\n",
      "2024-02-28 14:03:07 INFO:\u001b[32mTRUNCATION LENGTH: 4096\u001b[0m\n",
      "2024-02-28 14:03:07 INFO:\u001b[32mINSTRUCTION TEMPLATE: Llama-v2\u001b[0m\n",
      "2024-02-28 14:03:07 INFO:\u001b[32mLoaded the model in 12.83 seconds.\u001b[0m\n",
      "2024-02-28 14:03:23 INFO:\u001b[32mLoading JSON datasets...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** LoRA: dddd ***\n",
      "Testing Pytorch...\n",
      "\u001b[91mOverriding Torch checkpoint function to avoid repeated 'use_reentrant not explicitly set' warnings\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 69/69 [00:00<00:00, 2350.53 examples/s]\n",
      "2024-02-28 14:03:24 INFO:\u001b[32mGetting model ready...\u001b[0m\n",
      "2024-02-28 14:03:24 INFO:\u001b[32mPreparing for training...\u001b[0m\n",
      "2024-02-28 14:03:24 INFO:\u001b[32mCreating LoRA model...\u001b[0m\n",
      "2024-02-28 14:03:24 WARNING:\u001b[33mDetected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\u001b[0m\n",
      "2024-02-28 14:03:24 INFO:\u001b[32mStarting training...\u001b[0m\n",
      "2024-02-28 14:03:24 INFO:\u001b[32mLog file 'train_dataset_sample.json' created in the 'logs' directory.\u001b[0m\n",
      "wandb: Tracking run with wandb version 0.16.3\n",
      "wandb: W&B syncing is set to `offline` in this directory.  \n",
      "wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "2024-02-28 14:05:40 INFO:\u001b[32mLoRA training run is completed and saved.\u001b[0m\n",
      "2024-02-28 14:05:40 INFO:\u001b[32mTraining complete, saving...\u001b[0m\n",
      "2024-02-28 14:05:40 INFO:\u001b[32mTraining complete!\u001b[0m\n",
      "2024-02-28 14:06:02 INFO:\u001b[32mLoading Llama-2-7b-chat-hf...\u001b[0m\n",
      "2024-02-28 14:06:02 INFO:\u001b[32mUsing the following 4-bit params: {'load_in_4bit': True, 'bnb_4bit_compute_dtype': torch.float16, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': False}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS: True EOS: False\n",
      "Data Blocks: 69\n",
      "Transformers Model Type: \u001b[93mLlamaForCausalLM\u001b[0m\n",
      "Data Size Check: Gradient accumulation: \u001b[93m1\u001b[0m <= Blocks/Batch 17 ... \u001b[92m[OK]\u001b[0m\n",
      "Training 'llama' model using \u001b[93m(q, v)\u001b[0m projections\n",
      "Trainable params: 16,777,216 (\u001b[91m0.4770 %\u001b[0m), All params: 3,517,190,144 (Model: 3,500,412,928)\n",
      "\u001b[1;30;40mStep:      0 \u001b[0;37;0m{'loss': 3.6039, 'grad_norm': 2.4365620613098145, 'learning_rate': 2.9999999999999997e-06, 'epoch': 0.06}\n",
      "\u001b[1;30;40mStep:      1 \u001b[0;37;0m{'loss': 3.516, 'grad_norm': 2.6957333087921143, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.11}\n",
      "\u001b[1;30;40mStep:      2 \u001b[0;37;0m{'loss': 3.5105, 'grad_norm': 2.409376859664917, 'learning_rate': 8.999999999999999e-06, 'epoch': 0.17}\n",
      "\u001b[1;30;40mStep:      3 \u001b[0;37;0m{'loss': 3.635, 'grad_norm': 2.557298183441162, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.22}\n",
      "\u001b[1;30;40mStep:      4 \u001b[0;37;0m{'loss': 3.7029, 'grad_norm': 2.483044385910034, 'learning_rate': 1.4999999999999999e-05, 'epoch': 0.28}\n",
      "\u001b[1;30;40mStep:      5 \u001b[0;37;0m{'loss': 3.6556, 'grad_norm': 2.4022629261016846, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.33}\n",
      "\u001b[1;30;40mStep:      6 \u001b[0;37;0m{'loss': 3.2924, 'grad_norm': 2.4733359813690186, 'learning_rate': 2.1e-05, 'epoch': 0.39}\n",
      "\u001b[1;30;40mStep:      7 \u001b[0;37;0m{'loss': 3.3606, 'grad_norm': 2.528587818145752, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.44}\n",
      "\u001b[1;30;40mStep:      8 \u001b[0;37;0m{'loss': 3.3416, 'grad_norm': 2.5986216068267822, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.5}\n",
      "\u001b[1;30;40mStep:      9 \u001b[0;37;0m{'loss': 3.2723, 'grad_norm': 2.437180995941162, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.56}\n",
      "\u001b[1;30;40mStep:     10 \u001b[0;37;0m{'loss': 3.2342, 'grad_norm': 2.470118761062622, 'learning_rate': 3.2999999999999996e-05, 'epoch': 0.61}\n",
      "\u001b[1;30;40mStep:     11 \u001b[0;37;0m{'loss': 3.0991, 'grad_norm': 2.2877206802368164, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.67}\n",
      "\u001b[1;30;40mStep:     12 \u001b[0;37;0m{'loss': 2.9811, 'grad_norm': 2.1822433471679688, 'learning_rate': 3.9e-05, 'epoch': 0.72}\n",
      "\u001b[1;30;40mStep:     13 \u001b[0;37;0m{'loss': 2.7181, 'grad_norm': 2.2874608039855957, 'learning_rate': 4.2e-05, 'epoch': 0.78}\n",
      "\u001b[1;30;40mStep:     14 \u001b[0;37;0m{'loss': 2.9266, 'grad_norm': 2.0876200199127197, 'learning_rate': 4.4999999999999996e-05, 'epoch': 0.83}\n",
      "\u001b[1;30;40mStep:     15 \u001b[0;37;0m{'loss': 2.8544, 'grad_norm': 1.9274767637252808, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.89}\n",
      "\u001b[1;30;40mStep:     16 \u001b[0;37;0m{'loss': 2.6189, 'grad_norm': 1.9105294942855835, 'learning_rate': 5.1e-05, 'epoch': 0.94}\n",
      "\u001b[1;30;40mStep:     17 \u001b[0;37;0m{'loss': 2.2424, 'grad_norm': 2.1104788780212402, 'learning_rate': 5.399999999999999e-05, 'epoch': 1.0}\n",
      "\u001b[1;30;40mStep:     18 \u001b[0;37;0m{'loss': 2.35, 'grad_norm': 1.8402419090270996, 'learning_rate': 5.6999999999999996e-05, 'epoch': 1.06}\n",
      "\u001b[1;30;40mStep:     19 \u001b[0;37;0m{'loss': 2.239, 'grad_norm': 1.9262897968292236, 'learning_rate': 5.9999999999999995e-05, 'epoch': 1.11}\n",
      "\u001b[1;30;40mStep:     20 \u001b[0;37;0m{'loss': 2.1502, 'grad_norm': 1.8778345584869385, 'learning_rate': 6.299999999999999e-05, 'epoch': 1.17}\n",
      "\u001b[1;30;40mStep:     21 \u001b[0;37;0m{'loss': 2.1538, 'grad_norm': 1.8960659503936768, 'learning_rate': 6.599999999999999e-05, 'epoch': 1.22}\n",
      "\u001b[1;30;40mStep:     22 \u001b[0;37;0m{'loss': 1.9264, 'grad_norm': 1.9111862182617188, 'learning_rate': 6.9e-05, 'epoch': 1.28}\n",
      "\u001b[1;30;40mStep:     23 \u001b[0;37;0m{'loss': 1.9556, 'grad_norm': 1.8813202381134033, 'learning_rate': 7.199999999999999e-05, 'epoch': 1.33}\n",
      "\u001b[1;30;40mStep:     24 \u001b[0;37;0m{'loss': 1.8346, 'grad_norm': 1.8430390357971191, 'learning_rate': 7.5e-05, 'epoch': 1.39}\n",
      "\u001b[1;30;40mStep:     25 \u001b[0;37;0m{'loss': 1.7265, 'grad_norm': 1.8890942335128784, 'learning_rate': 7.8e-05, 'epoch': 1.44}\n",
      "\u001b[1;30;40mStep:     26 \u001b[0;37;0m{'loss': 1.4961, 'grad_norm': 1.7500832080841064, 'learning_rate': 8.1e-05, 'epoch': 1.5}\n",
      "\u001b[1;30;40mStep:     27 \u001b[0;37;0m{'loss': 1.3947, 'grad_norm': 2.3359174728393555, 'learning_rate': 8.4e-05, 'epoch': 1.56}\n",
      "\u001b[1;30;40mStep:     28 \u001b[0;37;0m{'loss': 1.2281, 'grad_norm': 2.0786612033843994, 'learning_rate': 8.699999999999999e-05, 'epoch': 1.61}\n",
      "\u001b[1;30;40mStep:     29 \u001b[0;37;0m Saved: [checkpoint-29-loss-1_23]\n",
      "\u001b[1;30;40mStep:     29 \u001b[0;37;0m{'loss': 1.1726, 'grad_norm': 2.2042460441589355, 'learning_rate': 8.999999999999999e-05, 'epoch': 1.67}\n",
      "\u001b[1;30;40mStep:     30 \u001b[0;37;0m{'loss': 1.0019, 'grad_norm': 1.9164034128189087, 'learning_rate': 9.3e-05, 'epoch': 1.72}\n",
      "\u001b[1;30;40mStep:     31 \u001b[0;37;0m{'loss': 0.8805, 'grad_norm': 2.10475754737854, 'learning_rate': 9.599999999999999e-05, 'epoch': 1.78}\n",
      "\u001b[1;30;40mStep:     32 \u001b[0;37;0m{'loss': 0.776, 'grad_norm': 1.8740038871765137, 'learning_rate': 9.9e-05, 'epoch': 1.83}\n",
      "\u001b[1;30;40mStep:     33 \u001b[0;37;0m{'loss': 0.6627, 'grad_norm': 2.230848789215088, 'learning_rate': 0.000102, 'epoch': 1.89}\n",
      "\u001b[1;30;40mStep:     34 \u001b[0;37;0m Saved: [checkpoint-34-loss-0_66]\n",
      "\u001b[1;30;40mStep:     34 \u001b[0;37;0m{'loss': 0.5587, 'grad_norm': 2.0825462341308594, 'learning_rate': 0.00010499999999999999, 'epoch': 1.94}\n",
      "\u001b[1;30;40mStep:     35 \u001b[0;37;0m{'loss': 0.5111, 'grad_norm': 5.747666835784912, 'learning_rate': 0.00010799999999999998, 'epoch': 2.0}\n",
      "\u001b[1;30;40mStep:     36 \u001b[0;37;0m{'loss': 0.4386, 'grad_norm': 2.3166191577911377, 'learning_rate': 0.00011099999999999999, 'epoch': 2.06}\n",
      "\u001b[1;30;40mStep:     37 \u001b[0;37;0m{'loss': 0.356, 'grad_norm': 1.428855061531067, 'learning_rate': 0.00011399999999999999, 'epoch': 2.11}\n",
      "\u001b[1;30;40mStep:     38 \u001b[0;37;0m Saved: [checkpoint-38-loss-0_36]\n",
      "\u001b[1;30;40mStep:     38 \u001b[0;37;0m{'loss': 0.4114, 'grad_norm': 3.0149643421173096, 'learning_rate': 0.000117, 'epoch': 2.17}\n",
      "\u001b[1;30;40mStep:     39 \u001b[0;37;0m{'loss': 0.3416, 'grad_norm': 1.9571056365966797, 'learning_rate': 0.00011999999999999999, 'epoch': 2.22}\n",
      "\u001b[1;30;40mStep:     40 \u001b[0;37;0m{'loss': 0.2887, 'grad_norm': 1.6127893924713135, 'learning_rate': 0.00012299999999999998, 'epoch': 2.28}\n",
      "\u001b[1;30;40mStep:     41 \u001b[0;37;0m{'loss': 0.3543, 'grad_norm': 2.078226327896118, 'learning_rate': 0.00012599999999999997, 'epoch': 2.33}\n",
      "\u001b[1;30;40mStep:     42 \u001b[0;37;0m{'loss': 0.3116, 'grad_norm': 1.0603177547454834, 'learning_rate': 0.000129, 'epoch': 2.39}\n",
      "\u001b[1;30;40mStep:     43 \u001b[0;37;0m{'loss': 0.2857, 'grad_norm': 0.9800037741661072, 'learning_rate': 0.00013199999999999998, 'epoch': 2.44}\n",
      "\u001b[1;30;40mStep:     44 \u001b[0;37;0m{'loss': 0.2704, 'grad_norm': 0.6396501660346985, 'learning_rate': 0.000135, 'epoch': 2.5}\n",
      "\u001b[1;30;40mStep:     45 \u001b[0;37;0m{'loss': 0.2931, 'grad_norm': 1.4041436910629272, 'learning_rate': 0.000138, 'epoch': 2.56}\n",
      "\u001b[1;30;40mStep:     46 \u001b[0;37;0m{'loss': 0.2506, 'grad_norm': 1.4910035133361816, 'learning_rate': 0.00014099999999999998, 'epoch': 2.61}\n",
      "\u001b[1;30;40mStep:     47 \u001b[0;37;0m{'loss': 0.2038, 'grad_norm': 1.2870147228240967, 'learning_rate': 0.00014399999999999998, 'epoch': 2.67}\n",
      "\u001b[1;30;40mStep:     48 \u001b[0;37;0m{'loss': 0.1872, 'grad_norm': 0.736460268497467, 'learning_rate': 0.000147, 'epoch': 2.72}\n",
      "\u001b[1;30;40mStep:     49 \u001b[0;37;0m{'loss': 0.1681, 'grad_norm': 1.7456055879592896, 'learning_rate': 0.00015, 'epoch': 2.78}\n",
      "\u001b[1;30;40mStep:     50 \u001b[0;37;0m Saved: [checkpoint-50-loss-0_17]\n",
      "\u001b[1;30;40mStep:     50 \u001b[0;37;0m{'loss': 0.0937, 'grad_norm': 1.1797900199890137, 'learning_rate': 0.00015299999999999998, 'epoch': 2.83}\n",
      "\u001b[1;30;40mStep:     51 \u001b[0;37;0m{'loss': 0.0473, 'grad_norm': 5.9886016845703125, 'learning_rate': 0.000156, 'epoch': 2.89}\n",
      "\u001b[1;30;40mStep:     52 \u001b[0;37;0m{'loss': 0.0414, 'grad_norm': 0.6318318843841553, 'learning_rate': 0.000159, 'epoch': 2.94}\n",
      "\u001b[1;30;40mStep:     53 \u001b[0;37;0m{'loss': 0.0448, 'grad_norm': 1.630789041519165, 'learning_rate': 0.000162, 'epoch': 3.0}\n",
      "\u001b[1;30;40mStep:     53 \u001b[0;37;0m{'train_runtime': 134.9096, 'train_samples_per_second': 1.534, 'train_steps_per_second': 0.4, 'train_loss': 1.6291153517172292, 'epoch': 3.0}\n",
      "Graph saved in loras/dddd/training_graph.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.19s/it]\n",
      "2024-02-28 14:06:11 INFO:\u001b[32mLOADER: Transformers\u001b[0m\n",
      "2024-02-28 14:06:11 INFO:\u001b[32mTRUNCATION LENGTH: 4096\u001b[0m\n",
      "2024-02-28 14:06:11 INFO:\u001b[32mINSTRUCTION TEMPLATE: Llama-v2\u001b[0m\n",
      "2024-02-28 14:06:11 INFO:\u001b[32mLoaded the model in 8.87 seconds.\u001b[0m\n",
      "2024-02-28 14:07:12 INFO:\u001b[32mApplying the following LoRAs to Llama-2-7b-chat-hf: dddd\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/g00cjz00/demo2024/llm_course_20231228/text-generation-webui/modules/callbacks.py\", line 57, in gentask\n",
      "    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n",
      "  File \"/home/g00cjz00/demo2024/llm_course_20231228/text-generation-webui/modules/text_generation.py\", line 352, in generate_with_callback\n",
      "    shared.model.generate(**kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1592, in generate\n",
      "    return self.sample(\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2696, in sample\n",
      "    outputs = self(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1168, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1008, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 734, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 635, in forward\n",
      "    value_states = self.v_proj(hidden_states)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/g00cjz00/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py\", line 290, in forward\n",
      "    output = lora_B(lora_A(dropout(x)))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# GET IP and Free Port\n",
    "# IP\n",
    "node_ip=$(cat /etc/hosts |grep \"$(hostname -a)\" | awk '{print $1}')\n",
    "# PORT\n",
    "noed_port_genai=$(python -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(('', 0)); addr = s.getsockname(); s.close(); print(addr[1])\")\n",
    "noed_port_genai=3000\n",
    "# PROXY\n",
    "proxy_url=/rstudio/${node_ip}/${noed_port_genai}\n",
    "# URL\n",
    "https_url=https://node01.biobank.org.tw${proxy_url}/\n",
    "echo $https_url\n",
    "\n",
    "# 強制刪除運行中服務\n",
    "ps -ef |grep server.py | awk '{print $2}' | xargs kill -9\n",
    "\n",
    "# EXPORT PATH for ~/.local\n",
    "export PATH=$PATH:$HOME/.local/bin\n",
    "\n",
    "# 主程式\n",
    "password=\"nchc:nchcorgtw\"\n",
    "cd text-generation-webui\n",
    "\n",
    "#HF_TOKEN='hf_TMvGGbIfHKROtIaOlWtzoUtjOTSMIQJZq' python server.py --api --api-port 9234 --api-key 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 --listen --listen-port ${noed_port_genai} --listen-host $(hostname -s) \\\n",
    "#--chat-buttons --public-api-id ${proxy_url} --gradio-auth ${password} --extensions Training_PRO \n",
    "\n",
    "HF_TOKEN='hf_TMvGGbIfHKROtIaOlWtzoUtjOTSMIQJZq' python server_nchc.py --api --api-port 9234 --listen --listen-port ${noed_port_genai} --listen-host $(hostname -s) \\\n",
    "--chat-buttons --public-api-id ${proxy_url} --gradio-auth ${password} --extensions Training_PRO \n",
    "\n",
    "\n",
    "#HF_TOKEN='hf_TMvGGbIfHKROtIaOlWtzoUtjOTSMIQJZq' python server.py --listen --listen-port ${noed_port_genai}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強制刪除服務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 強制刪除服務\n",
    "!ps -ef |grep server.py | awk '{print $2}' | xargs kill -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!strings /lib/x86_64-linux-gnu/libc.so.6 |grep GLIBC_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls /lib/x86_64-linux-gnu/libc.so.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo vi /etc/apt/sources.list\n",
    "1\n",
    "添加高版本的源\n",
    "deb http://th.archive.ubuntu.com/ubuntu jammy main    #添加该行到文件\n",
    "1\n",
    "运行升级\n",
    "sudo apt update\n",
    "sudo apt install libc6\n",
    "1\n",
    "2\n",
    "查看结果\n",
    "\n",
    "strings /lib/x86_64-linux-gnu/libc.so.6 |grep GLIBC_"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Image_S-work-textgen_c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-runtime",
   "language": "python",
   "name": "s-work-textgen_c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-runtime"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
