{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d0ea98-07c8-4577-90d5-a3baaf80c7e1",
   "metadata": {},
   "source": [
    "## Langchain Chain - OpenHermes-2.5-Mistral-7B-GPTQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5849f6-959b-417e-a20e-79816a0e7782",
   "metadata": {},
   "source": [
    "1. GPTQ\n",
    "- https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\n",
    "\n",
    "2. LANGCHAIN 手冊\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\n",
    "3. 學習LANGCHAIN -> CHAIN\n",
    "\n",
    "4. https://www.mlexpert.io/prompt-engineering/langchain-quickstart-with-llama-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bad7c7-2622-4b99-8189-a2bbb8550ca5",
   "metadata": {},
   "source": [
    "## 初始環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10aad2a-c43d-4203-bd42-6ab6936700c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fcf6a3-3c5d-45e9-bdb6-ca84dadeb418",
   "metadata": {},
   "source": [
    "## 確認CUDA版本, 以及否能使用GPU\n",
    "若無gpu 請點選右側->已連線->變更執行階段類型->T4 Gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9b510-7f4f-4091-b35d-2eda4df15ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847bbd94-c432-4f4f-a0ac-86a2e9916b6d",
   "metadata": {},
   "source": [
    "## 安裝套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011b984f-4b5d-4a94-a30e-b97dff0e5f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install chromadb cohere gdown kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q\n",
    "!pip install accelerate bitsandbytes hf_transfer huggingface_hub optimum transformers -q \n",
    "!pip install auto-gptq\n",
    "#!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  -q # Use cu117 if on CUDA 11.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ae0b1-a3a6-4376-9326-9a2e928a3388",
   "metadata": {},
   "source": [
    "### Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986de367-aa3c-458c-ac0f-e9c0747b49ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Download model\n",
    "mkdir -p /content/OpenHermes-2.5-Mistral-7B-GPTQ\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ --local-dir /content/OpenHermes-2.5-Mistral-7B-GPTQ --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d2113-d5e2-4e67-9113-5ab6223036f6",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "temperature 的參數值越小，模型就會回傳越確定的結果。如果調高該參數值，大語言模型可能會返回更隨機的結果，也就是說這可能會帶來更多樣化或更具創造性的產出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5423e-bfd9-4428-a4fa-8365c1bf6223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "MODEL_ID = \"/content/OpenHermes-2.5-Mistral-7B-GPTQ\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe, model_kwargs={\"temperature\": 0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654b8d7-606a-4ccc-ae1a-1be0d70f1e62",
   "metadata": {},
   "source": [
    "### 1. LLMChain (llm + prompt)\n",
    "LLMChain is most basic chain in Langchai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f629f-b9f8-4c1d-8fbf-c811d882a8e8",
   "metadata": {
    "id": "366f629f-b9f8-4c1d-8fbf-c811d882a8e8",
    "language": "python",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# PROMPT\n",
    "template=\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{question}[/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# 提問範例\n",
    "question = \"what are the 5 most populated cities in the world?\"\n",
    "print(\"\\n\\n提問範例\")\n",
    "print(prompt.format(question=question))\n",
    "\n",
    "# Create Chain (prompt + model)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# RUN CHAIN\n",
    "question = \"what are the 5 most populated cities in the world?\"\n",
    "print(\"\\n\\n提問內容\")\n",
    "print(chain.run(question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a302fec-2ae5-4c25-b86c-3bb01bd1853c",
   "metadata": {},
   "source": [
    "## 2. Sequential Chain  (llm + prompt)\n",
    "A sequential chain works by combining two or more chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05cfb7c-9c55-4670-9bcb-f1ae1d8b42e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load library\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# 1. TOPIC Template\n",
    "template=\"\"\"[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "Write a blog outline given a topic.\n",
    "Topic: {topic}[/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"topic\"])\n",
    "\n",
    "# TOPIC CHAIN\n",
    "topic_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"outline\")\n",
    "\n",
    "\n",
    "# 2. Outline Template\n",
    "template=\"\"\"[INST]Write a blog article based on the below outline.\n",
    "Outline: \n",
    "{outline}[/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"outline\"])\n",
    "\n",
    "# Outline CHAIN\n",
    "outline_chain = LLMChain(llm=llm, prompt=prompt, output_key=\"article\")\n",
    "\n",
    "\n",
    "# Sequential Chain\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[topic_chain, outline_chain],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"outline\", \"article\"],\n",
    "    verbose=True)\n",
    "\n",
    "# Chain Run\n",
    "result=overall_chain({\"topic\":\"台南旅遊規劃\"})\n",
    "\n",
    "# Result\n",
    "print(\"\\n\\n旅遊主題:\")\n",
    "print(result[\"topic\"])\n",
    "print(\"\\n\\n規劃大綱:\")\n",
    "print(result[\"outline\"])\n",
    "print(\"\\n\\n介紹文章:\")\n",
    "print(result[\"article\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf215bb0-3fd2-4460-838a-80df587316e5",
   "metadata": {},
   "source": [
    "## 3. Retrieval QA chain  (model + prompt + documents + vectordb)\n",
    "\n",
    "Retrieval QA chain is considered one of the most important helping with doing QA over your document data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c58cf3-c463-43c3-9940-149a7490c9d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p data/pdf/\n",
    "!gdown 1AldhEWVCtcE50XARgSnXR0azZ965nNmT -O data/pdf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c638c3d0-8b93-4a45-8f9f-dede4f2f8feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load library\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 文件解析\n",
    "pdf_file='./data/pdf/e2729e76-29a0-4be5-9eef-67809b05d6b9.pdf'\n",
    "loader= PyPDFLoader(pdf_file)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 向量資料庫\n",
    "Embeddings_ID=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings=HuggingFaceEmbeddings(model_name=Embeddings_ID)\n",
    "vectortdb = Chroma.from_documents(texts, embeddings)\n",
    "#DB_PATH = 'vectorstore/db_chroma'\n",
    "#vectortdb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=DB_PATH)\n",
    "\n",
    "# Load DB\n",
    "#embeddings = OpenAIEmbeddings()\n",
    "#DB_PATH = 'vectorstore/db_chroma'\n",
    "#vectortdb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)\n",
    "\n",
    "#: Test Search in Vector DB\n",
    "query = \"請說明櫃公司如何進行資產管理?\"\n",
    "source_documents=vectortdb.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(source_documents):\n",
    "    page_content=source_documents[i].page_content\n",
    "    page=source_documents[i].metadata[\"page\"]\n",
    "    source=source_documents[i].metadata[\"source\"]\n",
    "    file = os.path.basename(source) \n",
    "    print(\"Source: \"+file+\", Page \"+str(page+1) )\n",
    "    print(page_content)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08809294-9c39-4a87-ba0b-01547a84f2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ptompt template\n",
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Act as a cryptocurrency expert. Use the following information to answer the question at the end.\n",
    "<</SYS>>\n",
    " \n",
    "{context}\n",
    " \n",
    "{question} [/INST]\n",
    "\"\"\"\n",
    " \n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "#  RetrievalQA Chain 搜尋\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectortdb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},    \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Search\n",
    "query = \"請說明櫃公司如何進行資產管理?\"\n",
    "llm_response = chain(query)\n",
    "print(\"\\n\\n問題:\")\n",
    "print(llm_response['query'].strip())\n",
    "print(\"\\n\\n回答:\")\n",
    "print(llm_response['result'].strip())\n",
    "print(\"\\n\\n參考來源:\")\n",
    "print(llm_response['source_documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5474a-4f75-447a-b094-dc100c3665fd",
   "metadata": {},
   "source": [
    "## 4. RetrievalQAWithSourcesChain  (model + prompt + documents + vectordb)\n",
    "Retrieval QA chain is considered one of the most important helping with doing QA over your document data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da212c-3d30-4d1d-bfc3-8827e6fef9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load library\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 文件解析\n",
    "pdf_file='./data/pdf/e2729e76-29a0-4be5-9eef-67809b05d6b9.pdf'\n",
    "loader= PyPDFLoader(pdf_file)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# 向量資料庫\n",
    "Embeddings_ID=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings=HuggingFaceEmbeddings(model_name=Embeddings_ID)\n",
    "vectortdb = Chroma.from_documents(texts, embeddings)\n",
    "#DB_PATH = 'vectorstore/db_chroma'\n",
    "#vectortdb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=DB_PATH)\n",
    "\n",
    "# Load DB\n",
    "#embeddings = OpenAIEmbeddings()\n",
    "#DB_PATH = 'vectorstore/db_chroma'\n",
    "#vectortdb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)\n",
    "\n",
    "#: Test Search in Vector DB\n",
    "query = \"請說明櫃公司如何進行資產管理?\"\n",
    "source_documents=vectortdb.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(source_documents):\n",
    "    page_content=source_documents[i].page_content\n",
    "    page=source_documents[i].metadata[\"page\"]\n",
    "    source=source_documents[i].metadata[\"source\"]\n",
    "    file = os.path.basename(source) \n",
    "    print(\"Source: \"+file+\", Page \"+str(page+1) )\n",
    "    print(page_content)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4df5fd-527a-4af9-933d-b8fcd80c3d52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  RetrievalQAWithSourcesChain Chain 搜尋 + PROMPT\n",
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Act as a cryptocurrency expert. Use the following information to answer the question at the end.\n",
    "<</SYS>>\n",
    " \n",
    "{summaries}\n",
    " \n",
    "{question} [/INST]\n",
    "\"\"\"    \n",
    "    \n",
    "prompt = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
    "\n",
    "\n",
    "#  RetrievalQAWithSourcesChain 搜尋\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectortdb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt},    \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "# Search\n",
    "query = \"請說明櫃公司如何進行資產管理?\"\n",
    "llm_response = chain(query)\n",
    "print(\"\\n\\n問題:\")\n",
    "print(llm_response['question'])\n",
    "print(\"\\n\\n回答:\")\n",
    "print(llm_response['answer'])\n",
    "print(\"\\n\\n參考來源:\")\n",
    "print(llm_response['sources'])\n",
    "print(\"\\n\\n參考來源:\")\n",
    "print(llm_response['source_documents'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d0e336-9ff7-46be-bbfb-7c8af7fab6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RetrievalQAWithSourcesChain 頁碼解析\n",
    "import os\n",
    "source_documents=llm_response['source_documents'];\n",
    "\n",
    "for i, doc in enumerate(source_documents):\n",
    "    page_content=(llm_response['source_documents'][i].page_content)\n",
    "    page=(llm_response['source_documents'][i].metadata[\"page\"])\n",
    "    source=llm_response['source_documents'][i].metadata[\"source\"]\n",
    "    file = os.path.basename(source) \n",
    "    print(\"SOURCE: \"+file+\", PAGE: \"+str(page) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cf49bf-0819-494d-ad6c-14c4bbeaa029",
   "metadata": {},
   "source": [
    "## 5. Create Memory Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec5cbbe-7651-462f-af20-a7387890c6e0",
   "metadata": {
    "id": "bec5cbbe-7651-462f-af20-a7387890c6e0",
    "language": "python",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import ConversationChain, OpenAI, PromptTemplate, LLMChain\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff66c31-dee5-4ec7-a3b8-07b448ebe69c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dff66c31-dee5-4ec7-a3b8-07b448ebe69c",
    "language": "python",
    "outputId": "dab51cc7-e4af-48ca-e994-c0ad0c1e7db2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Customize the LLM template\n",
    "template = \"\"\"Assistant is a large language model trained by OpenAI.\n",
    "\n",
    "{history}\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"human_input\"], template=template)\n",
    "\n",
    "print(prompt.format(human_input=\"my_human_input\", history=\"my_history\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566dc258-9e1a-4724-9f9a-ce3a29e7dc96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "566dc258-9e1a-4724-9f9a-ce3a29e7dc96",
    "language": "python",
    "outputId": "241563f1-e429-4334-a8a0-96ca910e5b68",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create memory chain1\n",
    "chain = LLMChain(llm=llm,prompt=prompt,memory=ConversationBufferWindowMemory(k=2))\n",
    "\n",
    "# Predict a sentence using the chatgpt chain\n",
    "output = chain.run(human_input=\"請依序列出聯邦學習的重點\")\n",
    "\n",
    "# Display the model's response\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5788b0-e646-48ec-bf0c-dee3ac26ec43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa5788b0-e646-48ec-bf0c-dee3ac26ec43",
    "language": "python",
    "outputId": "2e54aedf-939e-48a9-f71a-5b769c85c750",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create memory chain2\n",
    "output = chain.run(human_input=\"請將以上的重點做一個結論\")\n",
    "\n",
    "# Display the model's response\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64c6e1-3d07-4e78-8be9-c6ccf8b35aa5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b64c6e1-3d07-4e78-8be9-c6ccf8b35aa5",
    "language": "python",
    "outputId": "a0fa7800-97b4-47bd-dd3d-0c3e146a6afe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create memory chain3\n",
    "output = chain.run(human_input=\"請將以上的總結, 規劃未來執行的方向\")\n",
    "\n",
    "# Display the model's response\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8327d-6355-454e-9452-f0b70eb9e1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupyterlab": {
   "notebooks": {
    "version_major": 6,
    "version_minor": 4
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "singlestore_cell_default_language": "python",
  "singlestore_connection": {
   "connectionID": "8bd78932-964d-4d20-91bc-72bfc2f211f6",
   "defaultDatabase": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
