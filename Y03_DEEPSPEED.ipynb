{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7367c70a-9c72-4326-be58-db358261629c",
   "metadata": {},
   "source": [
    "# Training\n",
    "1. Python\n",
    "2. Torchrun\n",
    "3. Accelerate\n",
    "4. Deepspeed\n",
    "5. Deepspeed + Torch.Distributed.Run\n",
    "\n",
    "```\n",
    "***** train metrics *****\n",
    "  epoch                    =        3.0\n",
    "  train_loss               =      0.948\n",
    "  train_runtime            = 1:25:14.22\n",
    "  train_samples_per_second =      2.933\n",
    "  train_steps_per_second   =      0.092\n",
    "Figure saved: saves/LLaMA2-7B-Chat/lora/04_sft18/training_loss.png\n",
    "01/03/2024 12:40:35 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60fe0b5-ef73-467b-acde-cde782c987bf",
   "metadata": {},
   "source": [
    "## 初始環境設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd7d4d6-9623-4b00-b2e0-cc9ffcc44e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 初始環境設定\n",
    "import os\n",
    "from pathlib import Path\n",
    "HOME = str(Path.home())\n",
    "Add_Binarry_Path=HOME+'/.local/bin:/usr/ubuntu_bin'\n",
    "os.environ['PATH']=os.environ['PATH']+':'+Add_Binarry_Path\n",
    "current_foldr=!pwd\n",
    "current_foldr=current_foldr[0]\n",
    "current_foldr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7356da-fc6b-40c0-b378-1f222b804c4c",
   "metadata": {},
   "source": [
    "## 套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297bc28-a1e6-42d1-a7f4-66b916fe32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cohere gdown kaleido langchain openai pyngrok pypdf python-dotenv sentence-transformers tiktoken -q\n",
    "!pip install accelerate bitsandbytes hf_transfer huggingface_hub optimum transformers==4.36.2 -q \n",
    "!pip install appdirs black black[jupyter] datasets fire loralib sentencepiece gradio==3.48.0 -q\n",
    "!pip install fastapi jieba matplotlib nltk peft==0.7.0 protobuf pydantic rouge-chinese scipy sse-starlette trl==0.7.6 uvicorn -q \n",
    "!pip install deepspeed -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74a472-de7a-4ab6-a420-d09c35c7adc6",
   "metadata": {},
   "source": [
    "## Method01 Python (singularity/notebook)\n",
    "- singularity: chage kernel to Python3 (ipykernel)\n",
    "- notebook: chage kernel to pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352b0c8-fabe-4b33-9235-3f9d95b99e80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "## Method01 Python (singularity/notebook)\n",
    "export GPUS_PER_NODE=1\n",
    "export MASTER_ADDR=$(hostname -s)\n",
    "export MASTER_PORT=$(python -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(('', 0)); addr = s.getsockname(); s.close(); print(addr[1])\")\n",
    "export MODEL_ID=\"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\"\n",
    "export DATASET=\"alpaca_gpt4_zh\"\n",
    "export OUTPUT_DIR=\"saves/LLaMA2-7B-Chat/lora/01_sft\"\n",
    "export MAX_SAMPLE=5000\n",
    "\n",
    "## CLEAN CACHE\n",
    "ps -ef |grep  train | awk '{print $2}' | xargs kill -9\n",
    "rm -rf ${OUTPUT_DIR}\n",
    "\n",
    "## RUN (chage kernel to Python3 or your Image kernel)\n",
    "## If you use Image kernel, mark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "## If you use python kernel, unmark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity run --nv --cleanenv -B /work -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/lib:/home/g00cjz00/.local/lib -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/bin:/home/g00cjz00/.local/bin -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin /work/u00cjz00/nvidia/pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv.sif \\\n",
    "bash -c \"export PATH=\\$PATH:\\$HOME/.local/bin; \\\n",
    "    cd /work/g00cjz00/github/LLaMA-Factory; \\\n",
    "    CUDA_VISIBLE_DEVICES=0 python \\\n",
    "    src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path ${MODEL_ID} \\\n",
    "    --dataset ${DATASET} \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir path_to_sft_checkpoint \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 1000 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16 \\\n",
    "    --max_samples ${MAX_SAMPLE} \\\n",
    "    --output_dir ${OUTPUT_DIR}\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6245d-4cb5-4154-a192-f4aa2c713894",
   "metadata": {},
   "source": [
    "## Method2 Torch.Run (singularity/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2bbfbb-4fef-4aa3-8a21-bb1fb96fc515",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "## Method2 Torch.Run (singularity/notebook)\n",
    "export GPUS_PER_NODE=8\n",
    "export MASTER_ADDR=$(hostname -s)\n",
    "export MASTER_PORT=$(python -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(('', 0)); addr = s.getsockname(); s.close(); print(addr[1])\")\n",
    "export MODEL_ID=\"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\"\n",
    "export DATASET=\"alpaca_gpt4_zh\"\n",
    "export OUTPUT_DIR=\"saves/LLaMA2-7B-Chat/lora/01_sft\"\n",
    "export MAX_SAMPLE=5000\n",
    "\n",
    "## CLEAN CACHE\n",
    "ps -ef |grep  train | awk '{print $2}' | xargs kill -9\n",
    "rm -rf ${OUTPUT_DIR}\n",
    "\n",
    "## RUN (chage kernel to Python3 or your Image kernel)\n",
    "## If you use Image kernel, unmark next two line, and mark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "## If you use python kernel, mark next two line, and unmark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "\n",
    "export SLURM_NNODES=1\n",
    "export SLURM_PROCID=0\n",
    "#/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity run --nv --cleanenv -B /work -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/lib:/home/g00cjz00/.local/lib -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/bin:/home/g00cjz00/.local/bin -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin /work/u00cjz00/nvidia/pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv.sif \\\n",
    "bash -c \"export PATH=\\$PATH:\\$HOME/.local/bin; \\\n",
    "    cd /work/g00cjz00/github/LLaMA-Factory; \\\n",
    "    torchrun --nproc_per_node=${GPUS_PER_NODE} --nnodes=${SLURM_NNODES} --master_port=${MASTER_PORT} \\\n",
    "    src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path ${MODEL_ID} \\\n",
    "    --dataset ${DATASET} \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir path_to_sft_checkpoint \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 1000 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16 \\\n",
    "    --max_samples ${MAX_SAMPLE} \\\n",
    "    --output_dir ${OUTPUT_DIR}\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc1351-c3ce-44fb-b44c-4632519dd5a2",
   "metadata": {},
   "source": [
    "## Method3 ACCELERATE (singularity/notebook)\n",
    "- singularity: chage kernel to Python3 (ipykernel)\n",
    "- notebook: chage kernel to pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23126ade-8816-49f7-8be2-df45f5e73548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << \\EOF >  accelerate.yml\n",
    "\n",
    "compute_environment: LOCAL_MACHINE\n",
    "distributed_type: MULTI_GPU\n",
    "downcast_bf16: 'no'\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "gpu_ids: 0,1,2,3,4,5,6,7\n",
    "num_machines: 1\n",
    "num_processes: 8\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f36033-aaf8-4bdc-925f-6440f048a85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "## Method3 ACCELERATE (singularity/notebook)\n",
    "export GPUS_PER_NODE=8\n",
    "export MASTER_ADDR=$(hostname -s)\n",
    "export MASTER_PORT=$(python -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(('', 0)); addr = s.getsockname(); s.close(); print(addr[1])\")\n",
    "export AC_CONFIG=\"accelerate.yml\"\n",
    "export MODEL_ID=\"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\"\n",
    "export DATASET=\"alpaca_gpt4_zh\"\n",
    "export OUTPUT_DIR=\"saves/LLaMA2-7B-Chat/lora/01_sft\"\n",
    "export MAX_SAMPLE=5000\n",
    "\n",
    "## CLEAN CACHE\n",
    "ps -ef |grep  train | awk '{print $2}' | xargs kill -9\n",
    "rm -rf ${OUTPUT_DIR}\n",
    "\n",
    "## RUN (chage kernel to Python3 or your Image kernel)\n",
    "## If you use Image kernel, mark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "## If you use python kernel, unmark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "\n",
    "#/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity run --nv --cleanenv -B /work -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/lib:/home/g00cjz00/.local/lib -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/bin:/home/g00cjz00/.local/bin -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin /work/u00cjz00/nvidia/pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv.sif \\\n",
    "bash -c \"export PATH=\\$PATH:\\$HOME/.local/bin; \\\n",
    "    cd /work/g00cjz00/github/LLaMA-Factory; \\\n",
    "    accelerate launch \\\n",
    "    --config_file ${AC_CONFIG} \\\n",
    "    src/train_bash.py \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path ${MODEL_ID} \\\n",
    "    --dataset ${DATASET} \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir path_to_sft_checkpoint \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 1000 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16 \\\n",
    "    --max_samples ${MAX_SAMPLE} \\\n",
    "    --output_dir ${OUTPUT_DIR}\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005a7603-9759-4cc6-b0c2-09c60a3fb8d0",
   "metadata": {},
   "source": [
    "## Method4 DEEPSPEED (singularity/notebook)\n",
    "- singularity: chage kernel to Python3 (ipykernel)\n",
    "- notebook: chage kernel to pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33370cc-7d21-4575-8280-4ee2bebe249d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "## Method4 DEEPSPEED (singularity/notebook)\n",
    "export GPUS_PER_NODE=2\n",
    "export MASTER_ADDR=$(hostname -s)\n",
    "export MASTER_PORT=$(python -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(('', 0)); addr = s.getsockname(); s.close(); print(addr[1])\")\n",
    "export DS_CONFIG=\"ds_config_zero3.json\"\n",
    "export MODEL_ID=\"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\"\n",
    "export DATASET=\"alpaca_gpt4_zh\"\n",
    "export OUTPUT_DIR=\"saves/LLaMA2-7B-Chat/lora/01_sft\"\n",
    "export MAX_SAMPLE=5000\n",
    "\n",
    "## CLEAN CACHE\n",
    "ps -ef |grep  train | awk '{print $2}' | xargs kill -9\n",
    "rm -rf ${OUTPUT_DIR}\n",
    "\n",
    "## RUN (chage kernel to Python3 or your Image kernel)\n",
    "## If you use Image kernel, mark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "## If you use python kernel, unmark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "\n",
    "#/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity run --nv --cleanenv -B /work -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/lib:/home/g00cjz00/.local/lib -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/bin:/home/g00cjz00/.local/bin -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin /work/u00cjz00/nvidia/pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv.sif \\\n",
    "bash -c \"export PATH=\\$PATH:\\$HOME/.local/bin; \\\n",
    "    cd /work/g00cjz00/github/LLaMA-Factory; \\\n",
    "    deepspeed \\\n",
    "    --hostfile=./hostfile \\\n",
    "    src/train_bash.py \\\n",
    "    --deepspeed ${DS_CONFIG} \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path ${MODEL_ID} \\\n",
    "    --dataset ${DATASET} \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir path_to_sft_checkpoint \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 1000 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16 \\\n",
    "    --max_samples ${MAX_SAMPLE} \\\n",
    "    --output_dir ${OUTPUT_DIR}\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee83be-ad63-4900-b325-3fd3f0571331",
   "metadata": {},
   "source": [
    "## Method5 DEEPSPEED + Torch.Distributed.Run (singularity/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682bc561-bfc3-41a4-b0c9-d952c4c589f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "## Method5 DEEPSPEED + Torch.Distributed.Run (singularity/notebook)\n",
    "export GPUS_PER_NODE=8\n",
    "export MASTER_ADDR=$(hostname -s)\n",
    "export MASTER_PORT=$(python -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(('', 0)); addr = s.getsockname(); s.close(); print(addr[1])\")\n",
    "export DS_CONFIG=\"ds_config_zero3.json\"\n",
    "export MODEL_ID=\"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\"\n",
    "export DATASET=\"alpaca_gpt4_zh\"\n",
    "export OUTPUT_DIR=\"saves/LLaMA2-7B-Chat/lora/01_sft\"\n",
    "export MAX_SAMPLE=5000\n",
    "\n",
    "\n",
    "## CLEAN CACHE\n",
    "ps -ef |grep  train | awk '{print $2}' | xargs kill -9\n",
    "rm -rf ${OUTPUT_DIR}\n",
    "\n",
    "## RUN (chage kernel to Python3 or your Image kernel)\n",
    "## If you use Image kernel, unmark next two line, and mark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "## If you use python kernel, mark next two line, and unmark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "\n",
    "export SLURM_NNODES=1\n",
    "export SLURM_PROCID=0\n",
    "#/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity run --nv --cleanenv -B /work -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/lib:/home/g00cjz00/.local/lib -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/bin:/home/g00cjz00/.local/bin -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin /work/u00cjz00/nvidia/pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv.sif \\\n",
    "bash -c \"export PATH=\\$PATH:\\$HOME/.local/bin; \\\n",
    "    cd /work/g00cjz00/github/LLaMA-Factory; \\\n",
    "    python3 -m torch.distributed.run \\\n",
    "    --nproc_per_node ${GPUS_PER_NODE} \\\n",
    "    --nnodes ${SLURM_NNODES} \\\n",
    "    --node_rank ${SLURM_PROCID} \\\n",
    "    --master_addr ${MASTER_ADDR} \\\n",
    "    --master_port ${MASTER_PORT} \\\n",
    "    src/train_bash.py \\\n",
    "    --deepspeed ${DS_CONFIG} \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path ${MODEL_ID} \\\n",
    "    --dataset ${DATASET} \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir path_to_sft_checkpoint \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 32 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16 \\\n",
    "    --max_samples ${MAX_SAMPLE} \\\n",
    "    --output_dir ${OUTPUT_DIR}\n",
    "\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49363e7c-6842-46b7-a5de-5767fd733234",
   "metadata": {},
   "source": [
    "## Method6 SRUN +  DEEPSPEED + Torch.Distributed.Run (singularity/notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4188171-adb5-4205-9ec8-7280f9e2c212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo $SLURM_NNODES\n",
    "echo $SLURM_PROCID\n",
    "echo $SLURM_JOBID\n",
    "#srun --jobid $SLURM_JOBID -w gn0815 bash -c \"echo $HOSTNAME\"\n",
    "srun --jobid 557377 -w gn0815 bash -c \"echo $SLURM_PROCID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b898cbf-ec53-4449-bdf8-31257b288cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "## Method6 SRUN + DEEPSPEED + Torch.Distributed.Run (singularity/notebook)\n",
    "export GPUS_PER_NODE=2\n",
    "export MASTER_ADDR=$(hostname -s)\n",
    "export MASTER_PORT=$(python -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(('', 0)); addr = s.getsockname(); s.close(); print(addr[1])\")\n",
    "export DS_CONFIG=\"ds_config_zero3.json\"\n",
    "export MODEL_ID=\"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\"\n",
    "export DATASET=\"alpaca_gpt4_zh\"\n",
    "export OUTPUT_DIR=\"saves/LLaMA2-7B-Chat/lora/01_sft\"\n",
    "export MAX_SAMPLE=5000\n",
    "\n",
    "\n",
    "export MASTER_ADDR=\"gn0814\"\n",
    "export MASTER_PORT=6000\n",
    "\n",
    "## CLEAN CACHE\n",
    "ps -ef |grep  train | awk '{print $2}' | xargs kill -9\n",
    "rm -rf ${OUTPUT_DIR}\n",
    "\n",
    "## RUN (chage kernel to Python3 or your Image kernel)\n",
    "## If you use Image kernel, unmark next two line, and mark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "## If you use python kernel, mark next two line, and unmark /work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity\n",
    "\n",
    "export SLURM_NNODES=2\n",
    "export SLURM_PROCID=1\n",
    "\n",
    "#srun --jobid $SLURM_JOBID \\\n",
    "/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity run --nv --cleanenv -B /work -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/lib:/home/g00cjz00/.local/lib -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/bin:/home/g00cjz00/.local/bin -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin /work/u00cjz00/nvidia/pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv.sif \\\n",
    "bash -c \"export PATH=\\$PATH:\\$HOME/.local/bin; \\\n",
    "    cd /work/g00cjz00/github/LLaMA-Factory; \\\n",
    "    python3 -m torch.distributed.run \\\n",
    "    --nproc_per_node ${GPUS_PER_NODE} \\\n",
    "    --nnodes ${SLURM_NNODES} \\\n",
    "    --node_rank ${SLURM_PROCID} \\\n",
    "    --master_addr ${MASTER_ADDR} \\\n",
    "    --master_port ${MASTER_PORT} \\\n",
    "    src/train_bash.py \\\n",
    "    --deepspeed ${DS_CONFIG} \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path ${MODEL_ID} \\\n",
    "    --dataset ${DATASET} \\\n",
    "    --template default \\\n",
    "    --finetuning_type lora \\\n",
    "    --lora_target q_proj,v_proj \\\n",
    "    --output_dir path_to_sft_checkpoint \\\n",
    "    --overwrite_cache \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 100 \\\n",
    "    --learning_rate 5e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --plot_loss \\\n",
    "    --fp16 \\\n",
    "    --max_samples ${MAX_SAMPLE} \\\n",
    "    --output_dir ${OUTPUT_DIR}\n",
    "\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6032d3-217f-4e70-b0b5-ace0f4f80030",
   "metadata": {},
   "source": [
    "## SLURM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82073a-2336-4709-a053-5841ccfb0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/work/u00cjz00/binary/bash5.0/bin/bash\n",
    "#SBATCH -A GOV109189                                                    ### project number, Example MST109178\n",
    "#SBATCH -J _t2demo_\t\t\t\t\t\t\t                            ### Job name, Exmaple jupyterlab\n",
    "#SBATCH -p gp4d                                                         ### Partition Name, Example ngs1gpu\n",
    "#SBATCH --nodes=2                                                       ### Nodes, Default 1, node number\n",
    "#SBATCH --ntasks-per-node=1                                             ### Tasks, Default 1, per node tasks\n",
    "#SBATCH -c 8                                                           ### Cores assigned to each task, Example 4\n",
    "#SBATCH --gres=gpu:2                                                    ### GPU number, Example gpu:1\n",
    "#SBATCH --time=0-1:00:00                                                ### Runnung time, days-hours:minutes:seconds or hours:minutes:seconds\n",
    "#SBATCH -o genai_%j.out     \t\t\t\t\t\t### Log folder, Here %j is job ID\n",
    "#SBATCH -e genai_%j.err     \t\t\t\t\t\t### Log folder, Here %j is job ID\n",
    "\n",
    "\n",
    "## 您的程式部分\n",
    "NODELIST=$(scontrol show hostname $SLURM_JOB_NODELIST)\n",
    "MASTER_NODE=$(head -n 1 <<< \"$NODELIST\")\n",
    "NODE_COUNT=0\n",
    "NODE_NUM=($(echo $NODELIST | tr \" \" \"\\n\" | wc -l))\n",
    "\n",
    "echo $SLURM_NODEID\n",
    "echo $NODELIST\n",
    "echo $MASTER_NODE\n",
    "echo $NODE_NUM\n",
    "export NCCL_IB_DISABLE=1\n",
    "export NCCL_SOCKET_IFNAME=bond0\n",
    "export CC=/opt/hpcx/ompi/bin/mpicc\n",
    "export CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "export GPUS_PER_NODE=2\n",
    "export MASTER_PORT=$(python -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.bind(('', 0)); addr = s.getsockname(); s.close(); print(addr[1])\")\n",
    "export DS_CONFIG=\"ds_config_zero3.json\"\n",
    "export MODEL_ID=\"/work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf\"\n",
    "export DATASET=\"alpaca_gpt4_zh\"\n",
    "export OUTPUT_DIR=\"saves/LLaMA2-7B-Chat/lora/01_sft\"\n",
    "export MAX_SAMPLE=5000\n",
    "\n",
    "rm -rf /work/g00cjz00/github/LLaMA-Factory/${OUTPUT_DIR}\n",
    "\n",
    "for NODE in $NODELIST; do\n",
    "    if [ \"$NODE\" == \"$MASTER_NODE\" ]; then\n",
    "\t\n",
    "cat << EOF >  ${NODE}.sh\n",
    "\n",
    "# RUN: ${NODE}.sh\n",
    "\n",
    "export PATH=\\$PATH:\\$HOME/.local/bin\n",
    "echo \\$PATH\n",
    "cd /work/g00cjz00/github/LLaMA-Factory\n",
    "python3 -m torch.distributed.run \\\n",
    "--nproc_per_node ${GPUS_PER_NODE} \\\n",
    "--nnodes ${SLURM_NNODES} \\\n",
    "--node_rank ${NODE_COUNT} \\\n",
    "--master_addr ${MASTER_NODE} \\\n",
    "--master_port ${MASTER_PORT} \\\n",
    "src/train_bash.py \\\n",
    "--deepspeed ${DS_CONFIG} \\\n",
    "--stage sft \\\n",
    "--do_train \\\n",
    "--model_name_or_path ${MODEL_ID} \\\n",
    "--dataset ${DATASET} \\\n",
    "--template default \\\n",
    "--finetuning_type lora \\\n",
    "--lora_target q_proj,v_proj \\\n",
    "--output_dir path_to_sft_checkpoint \\\n",
    "--overwrite_cache \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--lr_scheduler_type cosine \\\n",
    "--logging_steps 10 \\\n",
    "--save_steps 100 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--num_train_epochs 3.0 \\\n",
    "--plot_loss \\\n",
    "--fp16 \\\n",
    "--max_samples ${MAX_SAMPLE} \\\n",
    "--output_dir ${OUTPUT_DIR}\n",
    "\n",
    "EOF\n",
    "\n",
    "chmod 755 ${NODE}.sh \n",
    "sleep 5\n",
    "srun --jobid $SLURM_JOBID --nodes=1 --ntasks=1 -w $NODE \\\n",
    "/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity run --nv --cleanenv -B /work -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/lib:/home/g00cjz00/.local/lib -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/bin:/home/g00cjz00/.local/bin -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin /work/u00cjz00/nvidia/pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv.sif \\\n",
    "./${NODE}.sh &\n",
    "\t\t\n",
    "    else\n",
    "        ((NODE_COUNT++))\n",
    "\n",
    "cat << EOF >  ${NODE}.sh\n",
    "\n",
    "# RUN: ${NODE}.sh\n",
    "\n",
    "export PATH=\\$PATH:\\$HOME/.local/bin\n",
    "echo \\$PATH\n",
    "cd /work/g00cjz00/github/LLaMA-Factory\n",
    "python3 -m torch.distributed.run \\\n",
    "--nproc_per_node ${GPUS_PER_NODE} \\\n",
    "--nnodes ${SLURM_NNODES} \\\n",
    "--node_rank ${NODE_COUNT} \\\n",
    "--master_addr ${MASTER_NODE} \\\n",
    "--master_port ${MASTER_PORT} \\\n",
    "src/train_bash.py \\\n",
    "--deepspeed ${DS_CONFIG} \\\n",
    "--stage sft \\\n",
    "--do_train \\\n",
    "--model_name_or_path ${MODEL_ID} \\\n",
    "--dataset ${DATASET} \\\n",
    "--template default \\\n",
    "--finetuning_type lora \\\n",
    "--lora_target q_proj,v_proj \\\n",
    "--output_dir path_to_sft_checkpoint \\\n",
    "--overwrite_cache \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--lr_scheduler_type cosine \\\n",
    "--logging_steps 10 \\\n",
    "--save_steps 100 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--num_train_epochs 3.0 \\\n",
    "--plot_loss \\\n",
    "--fp16 \\\n",
    "--max_samples ${MAX_SAMPLE} \\\n",
    "--output_dir ${OUTPUT_DIR}\n",
    "\n",
    "EOF\n",
    "\n",
    "chmod 755 ${NODE}.sh \n",
    "sleep 5\n",
    "\n",
    "srun --jobid $SLURM_JOBID --nodes=1 --ntasks=1 -w $NODE \\\n",
    "/work/opt/ohpc/Taiwania3/libs/singularity/3.10.2/bin/singularity run --nv --cleanenv -B /work -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/lib:/home/g00cjz00/.local/lib -B /work/g00cjz00/libraryFolder/S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv/local/bin:/home/g00cjz00/.local/bin -B /work/u00cjz00/os/ubuntu/bin:/usr/ubuntu_bin /work/u00cjz00/nvidia/pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv.sif \\\n",
    "./${NODE}.sh &\n",
    "\t\t\n",
    "    fi\n",
    "done\n",
    "wait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537f999-3959-4ba8-9fcc-27bac5098074",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e9633-cd84-4e33-ac4d-3c2b183eb640",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT=/path/to/output\n",
    "export GPUS_PER_NODE=8\n",
    "\n",
    "TASK_NAME=\"task_name\"\n",
    "export CKPT=\"$OUTPUT/$TASK_NAME\"\n",
    "mkdir -p $OUTPUT $CKPT \n",
    "\n",
    "echo \"SLURM_NTASKS=$SLURM_NTASKS\"\n",
    "export HOSTFILE=\"./hostfile\"\n",
    "echo \"[slurm node name] slots=$SLURM_NTASKS\" > $HOSTFILE\n",
    "\n",
    "### 定义节点到ip的映射\n",
    "declare -A IB0\n",
    "for i in $(seq 0 node_count); do\n",
    "    key=$(printf \"NODE_NAME%02d\" \"$i\")\n",
    "    value=$(printf \"xx.xx.xx.%02d\" $((i)))\n",
    "    IB0[\"$key\"]=\"$value\"\n",
    "done\n",
    "### 这一步定义你的slurm集群中所有的节点与ip的对应关系，用一个dict存储\n",
    "\n",
    "### 系统自动匹配当前进程的主节点\n",
    "regex=\"[0-9]+\"\n",
    "if [[ $SLURM_JOB_NODELIST =~ $regex ]]; then\n",
    "    extracted_number=\"${BASH_REMATCH[0]}\"\n",
    "    echo \"Extracted number: $extracted_number\"\n",
    "else\n",
    "    echo \"No match found\"\n",
    "fi\n",
    "\n",
    "MASTER_KEY=\"NODE_NAME$extracted_number\"\n",
    "\n",
    "### 定义master的地址和端口\n",
    "export MASTER_ADDR=${IB0[$MASTER_KEY]}\n",
    "export MASTER_PORT=31217\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c6124-754f-4a42-803a-d3e4f4a5e235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "declare -A IB0\n",
    "node_count=\"1\"\n",
    "key=$(printf \"NODE_NAME%02d\" \"1\")\n",
    "value=$(printf \"xx.xx.xx.%02d\" $((i)))\n",
    "IB0[\"$key\"]=\"$value\"\n",
    "echo $value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1e9af-b970-4a7c-97ab-38d278aa38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash \n",
    "#SBATCH -J gpt_zh_en                     #作业名\n",
    "#SBATCH -p A800:8                         #使用的机器的名字，这个机器里面有gpu1和gpu2两个节点\n",
    "#SBATCH -w gpu2                           #具体使用的节点名为gpu2，需要注意，不能再使用--nodes=1\n",
    "#SBATCH --ntasks-per-node=12        #每个计算节点上使用srun启动的任务数\n",
    "#SBATCH --time 720:00:00               #运行时间720h约等于1个月\n",
    "#SBATCH --mem=240G                    #运行时间720h约等于1个月\n",
    "#SBATCH --comment=BASE              #附加注释信息\n",
    "#SBATCH -o /public1/home/amzhou/slurm/std.out.%j     #输出文件路径，带有作业ID\n",
    "#SBATCH -e /public1/home/amzhou/slurm/std.err.%j      #报错文件路径，带有作业ID\n",
    "#SBATCH --gres=gpu:8 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db138b3f-0e1f-49e2-84de-8ded5cbb6cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "WORK_DIR=/tmp\n",
    "### Set basic var   ### MARK_slurm2pbs                    #设置变量\n",
    "JOBID=$SLURM_JOB_ID                                  ### slurm2pbs             #声明作业ID变量\n",
    "NP=$SLURM_NPROCS                                     ### slurm2pbs              #声明进程数\n",
    "NNODE=$(srun hostname | sort | uniq | wc -l)          ### slurm2pbs        #统计唯一节点数\n",
    "LOG_FILE=$WORK_DIR/job_${JOB_NAME}_${JOBID}.log                       #定义日志文件路径\n",
    "#HOST_FILE=$WORK_DIR/job_${JOB_NAME}_${JOBID}_${NP}c_${NNODE}n.ma # 定义主机文件路径 \n",
    "#srun hostname | sort | uniq -c |awk '{print $2\":\"$1}' > $HOST_FILE  ### slurm2pbs    # 创建包含节点计数的文件\n",
    "### Write basic job infomations           #正常输出一些信息\n",
    "echo -e \"The start time is: `date +\"%Y-%m-%d %H:%M:%S\"` \\n\" | tee -a $LOG_FILE \n",
    "echo -e \"My job ID is: $JOBID \\n\" | tee -a $LOG_FILE  \n",
    "echo -e \"The total cores is: $NP \\n\" | tee -a $LOG_FILE \n",
    "echo -e \"The hosts is: \\n\" | tee -a $LOG_FILE\n",
    "#cat $HOST_FILE | tee -a $LOG_FILE\n",
    "echo -e \"\\n\"  | tee -a $LOG_FILE\n",
    "### Run APP                                     #准备运行自己的东西\n",
    "# MARK_CMD  #Don't delete this line!!! #这行标记CMD\n",
    "#!/bin/bash   \n",
    "echo $JOBID\n",
    "echo $NP\n",
    "echo $NNODE\n",
    "echo $LOG_FILE\n",
    "echo $JOBID\n",
    "#echo $HOST_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d7966-c4dc-43df-a044-a3a36f15b9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!srun hostname | sort | uniq | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b28114b-2507-479b-98d7-41548217b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "export GPUS_PER_NODE=8\n",
    "export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n",
    "export MASTER_PORT=9901\n",
    "\n",
    "srun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \\\n",
    " --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \\\n",
    " --master_addr $MASTER_ADDR --master_port $MASTER_PORT \\\n",
    "your_program.py <normal cl args> --deepspeed ds_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2188a0b4-fc25-41a7-b12f-de7057943ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ps -ef |grep  train | awk '{print $2}' | xargs kill -9\n",
    "!rm -rf saves/LLaMA2-7B-Chat/lora/01_sft\n",
    "!python3 -m torch.distributed.run --nproc_per_node 2 --nnodes 2 --node_rank 0 --master_addr gpn3001 --master_port 45797 src/train_bash.py --deepspeed ds_config_zero3.json --stage sft --do_train --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf --dataset alpaca_gpt4_zh --template default --finetuning_type lora --lora_target q_proj,v_proj --output_dir path_to_sft_checkpoint --overwrite_cache --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_steps 100 --learning_rate 5e-5 --num_train_epochs 3.0 --plot_loss --fp16 --max_samples 1000 --output_dir saves/LLaMA2-7B-Chat/lora/01_sft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a5531b-64de-4e61-8057-ded4f70cf62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps -ef |grep  train | awk '{print $2}' | xargs kill -9\n",
    "!rm -rf saves/LLaMA2-7B-Chat/lora/01_sft\n",
    "!python3 -m torch.distributed.run --nproc_per_node 1 --nnodes 2 --node_rank 1 --master_addr gpn3001 --master_port 45797 src/train_bash.py --deepspeed ds_config_zero3.json --stage sft --do_train --model_name_or_path /work/u00cjz00/slurm_jobs/github/models/Llama-2-7b-chat-hf --dataset alpaca_gpt4_zh --template default --finetuning_type lora --lora_target q_proj,v_proj --output_dir path_to_sft_checkpoint --overwrite_cache --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_steps 100 --learning_rate 5e-5 --num_train_epochs 3.0 --plot_loss --fp16 --max_samples 5000 --output_dir saves/LLaMA2-7B-Chat/lora/01_sft\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Image_S_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv",
   "language": "python",
   "name": "s_work-genai11_pytorch_2.1.2-cuda11.8-cudnn8-devel_opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
